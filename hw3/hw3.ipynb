{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLG454E - Learning From Data, Homework 3\n",
    "\n",
    "In this homework, you are supposed to implement following parts:\n",
    "\n",
    " -  **Part 1: SVM and multi-class softmax (40 points)**\n",
    "     - Refer to Machine Learning Blinks 8 and 9 for this part.\n",
    "     - ML Blinks 8: https://www.youtube.com/watch?v=KWp_TxnWfSU&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     - ML Blinks 9: https://www.youtube.com/watch?v=7bIVFiKpMfg&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     \n",
    "     \n",
    " - **Part 2: practice feature selection (20 points)**\n",
    "     - Refer to Machine Learning Blinks 10 for this part:\n",
    "     - ML Blinks 10: https://www.youtube.com/watch?v=laeth5oT9YM&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     \n",
    "     \n",
    " - **Part 3: solve an SVM optimization problem by hand (40 points)**\n",
    " \n",
    " \n",
    " ### Important Notes:\n",
    "   - Please complete this template and include any other necessary materials (screenshots of your handwritten solutions etc.) into the HW3 folder. Then zip it again and submit to Ninova.\n",
    "   - For Part 1, you need to implement the required functions and gradients etc. by yourself. Do not use autograd or any built-in functions.\n",
    "   - For the Part 2, you can use scikit-learn built-in functions for training an SVM and feature selection. \n",
    "   - At Part 3, you can upload the screenshots of your handwritten solutions to the Notebook. But please be sure that your solutions are neat and can be read properly.\n",
    "   - You can ask your questions on Ninova message board under HW3 header (recommended) or send an e-mail to akti15@itu.edu.tr.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Code up gradient descent for the soft-margin SVM loss (20 points)\n",
    "\n",
    "We have seen the margin perceptron and its implementation at previous lectures. SVM loss also uses the margin loss as base and tries to define and maximize a margin between classes as the following figure shows.\n",
    "\n",
    "\n",
    "<img src=\"images/part1-1.png\" width = \"450\">\n",
    "\n",
    "\n",
    "The soft-margin SVM loss with regularization can be calculated using following equation.\n",
    "\n",
    "<br>\n",
    "<center> $g(b, w) = \\sum_{p=1}^{P} max(0, 1 - y_{p}(b + x_{p}^T w)) + \\lambda\\left \\| w \\right \\|_2^2$ </center>\n",
    "<br>\n",
    "\n",
    "The following template creates w and x vectors as $ \\tilde{x}_{p} = \\begin{bmatrix} 1\\\\x_{p}\\end{bmatrix}^T$ and $\\tilde{w} = \\begin{bmatrix}b\\\\w \\end{bmatrix} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function loads data, creates X and y and adds 1's to X vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data \n",
    "def load_data(csvname):\n",
    "    # load in data\n",
    "    data = np.asarray(pd.read_csv(csvname,header = None))\n",
    "\n",
    "    X = data[:,0:-1]\n",
    "    y = data[:,-1]\n",
    "    y.shape = (len(y),1)\n",
    "    \n",
    "    # pad data with ones for more compact gradient computation\n",
    "    o = np.ones((np.shape(X)[0],1))\n",
    "    X = np.concatenate((o,X),axis = 1)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** The following function finds the optimal w values for the given input. The parameters of the function are samples in X, labels in y and the regularization parameter $\\lambda$. You need to calculate the gradient of the given soft-margin SVM loss with regularization function and apply gradient descent (you can use Vanilla gradient descent here). **Do not forget to add the regularization term.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE ###\n",
    "# gradient descent function for softmax cost/logistic regression \n",
    "def soft_svm_gradient_descent(X,y,lam):\n",
    "    # Initializations \n",
    "    w = np.random.randn(3,1);        # random initial point\n",
    "    alpha = 10**-2\n",
    "    max_its = 2000\n",
    "    \n",
    "    for k in range(max_its):\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** The following plot function plots the data points and the separating line. Here, you need to add $x_1$ and $x_2$ lines to the plot in order to see the margin as shown in Fig 4.14 above. Do not forget that $x_1$ and $x_2$ are parallel to the found separator and the distance between them is $\\frac{2}{\\left \\| w \\right \\|_2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE ###\n",
    "# plots everything \n",
    "def plot_all(X,y,w,lam):\n",
    "    # custom colors for plotting points\n",
    "    red = [1,0,0.4]  \n",
    "    blue = [0,0.4,1]\n",
    "    \n",
    "    # scatter plot points\n",
    "    fig = plt.figure(figsize = (4,4))\n",
    "    ind = np.argwhere(y==1)\n",
    "    ind = [s[0] for s in ind]\n",
    "    plt.scatter(X[ind,1],X[ind,2],,color = red,edgecolor = 'k',s = 25)\n",
    "    ind = np.argwhere(y==-1)\n",
    "    ind = [s[0] for s in ind]\n",
    "    plt.scatter(X[ind,1],X[ind,2],,color = blue,edgecolor = 'k',s = 25)\n",
    "    plt.grid('off')\n",
    "    \n",
    "    # plot separator\n",
    "    s = np.linspace(-1,1,100) \n",
    "    plt.plot(s,(-w[0]-w[1]*s)/w[2],color = 'k',linewidth = 2)\n",
    "    \n",
    "    # clean up plot\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.title('soft-margin svm with lambda = ' + str(lam))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you need to run 5 experiments which are:\n",
    "   - No regularization\n",
    "   - Regularization with $\\lambda = 10^{-2}$\n",
    "   - Regularization with $\\lambda = 10^{-1}$\n",
    "   - Regularization with $\\lambda = 1$\n",
    "   - Regularization with $\\lambda = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "X,y = load_data('imbalanced_2class.csv')\n",
    "\n",
    "# run soft-margin svm over a range of regularization parameters lambda \n",
    "lams = [0, 10**-2, 10**-1, 1, 10]\n",
    "for lam in lams:\n",
    "    # run gradient descent\n",
    "    w = soft_svm_gradient_descent(X,y,lam)\n",
    "\n",
    "    # plot points and separator\n",
    "    plot_all(X,y,w,lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Answer the following questions.\n",
    " - Why do we need regularization?\n",
    " - What is the effect of the $\\lambda$ on regularization?\n",
    " - Compare and discuss your results at previous step, what is the optimum $\\lambda$ value?\n",
    " - How do we choose $\\lambda$ parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Code up gradient descent for the multiclass softmax classier (20 points)\n",
    "\n",
    "In this exercise you will code up gradient descent to minimize the multi-class softmax cost function on a toy dataset, reproducing the result shown in Fig. 4.20.\n",
    "\n",
    "<img src=\"images/part1-2.png\" width =\"500\">\n",
    "\n",
    "Where $\\tilde{x}_{p} = \\begin{bmatrix} 1\\\\x_{p}\\end{bmatrix}$ and $\\tilde{w} = \\begin{bmatrix}b\\\\w \\end{bmatrix} $, $p$ is the sample number and $c$ is the class number, the compact version of the multi-class softmax loss is:\n",
    "\n",
    "<br>\n",
    "<center> $g(\\tilde{w}_1, ... , \\tilde{w}_C) = \\sum_{c=1}^{C} \\sum_{p\\in \\Omega_{c}} log ( 1 + \\sum_{j=1, j\\neq c} e^{\\tilde{x}_p^T}(\\tilde{w}_j - \\tilde{w}_c))$ </center>\n",
    "    \n",
    "<br>    \n",
    "\n",
    "Here, $p\\in \\Omega_{c}$ means the samples that belongs to class c. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads the data. There will be 4 classes in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data \n",
    "def load_data(csvname):\n",
    "    # load in data\n",
    "    data = np.asarray(pd.read_csv(csvname))\n",
    "\n",
    "    # import data and reshape appropriately\n",
    "    X = data[:,0:-1]\n",
    "    y = data[:,-1]\n",
    "    y.shape = (len(y),1)\n",
    "    \n",
    "    # pad data with ones for more compact gradient computation\n",
    "    o = np.ones((np.shape(X)[0],1))\n",
    "    X = np.concatenate((o,X),axis = 1)\n",
    "    X = X.T\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you are supposed to calculate the gradient of the multi-class softmax loss function. Please note that we are calculating a W vector for each of the classes. Thus, the gradient will also has the shape of $(D, C)$ where $N$ is the number of samples and D is the number of features including the bias term. \n",
    "\n",
    "<center> $W = \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ . \\\\ . \\\\ . \\\\ w_{C} \\end{bmatrix}^{T} $ and $ \\nabla g = \\begin{bmatrix} \\nabla_{w_{1}} g \\\\ \\nabla_{w_{2}} g \\\\ . \\\\ . \\\\ . \\\\ \\nabla_{w_{C}} g\\end{bmatrix}^{T}$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: YOUR CODE GOES HERE ###\n",
    "# learn all C separators running gradient descent\n",
    "def gradient_descent(x,y,alpha):\n",
    "    # formulate full input data matrix X\n",
    "    W = np.random.randn(3,C)\n",
    "\n",
    "    # gradient descent loop\n",
    "    max_its = 1000\n",
    "    for k in range(max_its):\n",
    "\n",
    "        \n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots the datapoints and all of the separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data, separators, and fused fule\n",
    "def plot_all(X,y,W):\n",
    "    # initialize figure, plot data, and dress up panels with axes labels etc.\n",
    "    num_classes = np.size(np.unique(y))\n",
    "    color_opts = np.array([[1,0,0.4], [ 0, 0.4, 1],[0, 1, 0.5],[1, 0.7, 0.5],[0.7, 0.6, 0.5]])\n",
    "    f,axs = plt.subplots(1,3,facecolor = 'white',figsize = (10,3))\n",
    "    for a in range(0,3):\n",
    "        for i in range(0,num_classes):\n",
    "            s = np.argwhere(y == i+1)\n",
    "            s = s[:,0]\n",
    "            axs[a].scatter(X[1,s],X[2,s], s = 30,color = color_opts[i,:])\n",
    "\n",
    "        # dress panel correctly\n",
    "        axs[a].set_xlim(0,1)\n",
    "        axs[a].set_ylim(0,1)\n",
    "        axs[a].axis('off')\n",
    "\n",
    "    r = np.linspace(0,1,150)\n",
    "    for i in range(0,num_classes):\n",
    "        z = -W[0,i]/W[2,i] - W[1,i]/W[2,i]*r\n",
    "        axs[1].plot(r,z,'-k',linewidth = 2,color = color_opts[i,:])\n",
    "\n",
    "    # fuse individual subproblem separators into one joint rule\n",
    "    r = np.linspace(0,1,300)\n",
    "    s,t = np.meshgrid(r,r)\n",
    "    s = np.reshape(s,(np.size(s),1))\n",
    "    t = np.reshape(t,(np.size(t),1))\n",
    "    h = np.concatenate((np.ones((np.size(s),1)),s,t),1)\n",
    "    f = np.dot(W.T,h.T)\n",
    "    z = np.argmax(f,0)\n",
    "    f.shape = (np.size(f),1)\n",
    "    s.shape = (np.size(r),np.size(r))\n",
    "    t.shape = (np.size(r),np.size(r))\n",
    "    z.shape = (np.size(r),np.size(r))\n",
    "\n",
    "    for i in range(0,num_classes + 1):\n",
    "        axs[2].contour(s,t,z,num_classes-1,colors = 'k',linewidths = 2.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see the results on 4 class data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X,y = load_data('four_class_data.csv')\n",
    "\n",
    "# perform gradient descent on softmax multiclass\n",
    "alpha = 10**(-2)    # step length, tune to your heat's desire!\n",
    "W = gradient_descent(X,y,alpha)           # learn all C vs notC separators\n",
    "plot_all(X,y,W)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature selection using scikit-learn (20 points)\n",
    "\n",
    "In this part, we will use scikit-learn library. You can install the necessary package using following commands:\n",
    "\n",
    "        > python3 -m pip install scikit-learn\n",
    "        > conda install -c conda-forge scikit-learn\n",
    "        \n",
    "There are lots of machine learning techniques that are available in scikit-learn library. In this problem we will use the **SVM** classifier and two feature selection methods which are **Recursive Feature Elimination** and **Select K Best**. You can check the documentations on the internet to learn how to use these functions and which parameters to use. Necessary functions are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are loading the handwritten numbers dataset. Then the data is splitted into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the data:  1797\n",
      "Number of features in the data:  64\n",
      "Classes in the data:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Number of training samples:  1437\n",
      "Number of testing samples:  360\n"
     ]
    }
   ],
   "source": [
    "X, y = load_digits().data, load_digits().target\n",
    "\n",
    "print(\"Number of samples in the data: \", X.shape[0])\n",
    "print(\"Number of features in the data: \", X.shape[1])\n",
    "print(\"Classes in the data: \", np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True )\n",
    "\n",
    "print(\"Number of training samples: \", X_train.shape[0])\n",
    "print(\"Number of testing samples: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** In the following cell, fill in the training and testing functions using SVM classifier (SVC function). Then, use this functions in the following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return classifier, train_accuracy\n",
    "\n",
    "def test(classifier, X, y):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return test_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Experiment 1: Train an SVM classifier **without** applying feature selection on the data and then test it on test set. Print the training accuracy and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier without feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4 points)** Experiment 2: Train an SVM classifier using **RFE (Recursive Feature Selector)** as feature selection method. You can use SVC as the estimator. Select your features based on the X_train and y_train, then define your new train and test sets using the features that you selected. Try to use different values for selected number of features and find the best resulted one. Print the number of selected features, training accuracy and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier with RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4 points)** Experiment 3: Train an SVM classifier using **Seleck K Best** as feature selection method. Select your features based on the X_train and y_train, then define your new train and test sets using the features that you selected. Print the number of selected features, training accuracy and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM classifier with Select K Best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Answer following questions based on your results.\n",
    "\n",
    "   - Which approach is better (with or without feature selection)? Why?\n",
    "   - What is the effect of number of selected features on the performance?\n",
    "   - What type of feature selection methods we have used (the types of RFE and SelectKBest)? What is the main difference between them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Solving SVM optimization by hand (40 points)\n",
    "\n",
    "You can insert the screenshots of your handwritten solution on Jupyter Notebook. For an example, check the cells including images. Do not forget to include your solution image file into the submitted .zip file.\n",
    "\n",
    "Some reminders for the question:\n",
    "\n",
    " - Lagrangian to optimize: $\\mathcal{L}_{primal} = \\sum_{i=1}^{n} a_{i} - \\frac{1}{2} [\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x^{j}] $ \n",
    "\n",
    "\n",
    "- Constraint: $\\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0$\n",
    "\n",
    "\n",
    "- Optimal parameter: $w^{*} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} x^{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 (20 points)\n",
    "\n",
    "<p style=\"float: left;\"><img src=\"images/part3-1.png\" width = \"200\"></p>\n",
    "        \n",
    "        Given the two following training samples (n=2), provide below a step-by-step solution\n",
    "        to estimate the optimal parameters (w and b) of the hyperplane separating the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 (10 points) \n",
    "If we add a third training point $x^3 = \\left [\\begin{matrix} 4 \\\\ 4 \\end{matrix}\\right] $, will that impact the hyperplane estimated using points $x^1$ and $x^2$? Justify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3 (10 points)\n",
    "Explain how to classify the point $x^{test} = \\left [\\begin{matrix} -1 \\\\ 0 \\end{matrix}\\right] $ using the estimated model. What is the predicted label of $x^{test}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bit0c47a300c4354c40b9deba433750818a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
