{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling Task (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the dataset object and and create dataloaders from it. Then you need to implement network models, training loops and evaluation and generation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in required parts of code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3992,
     "status": "ok",
     "timestamp": 1609426366377,
     "user": {
      "displayName": "Fırat Öncel",
      "photoUrl": "",
      "userId": "09411010771070540721"
     },
     "user_tz": -180
    },
    "id": "xk97LrSrjOOO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "import pdb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CE3SB2yTkUPs"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ixcE5OBNjOOR"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        path,\n",
    "        train=True,\n",
    "        train_split=0.8,\n",
    "        device='cuda'\n",
    "    ):\n",
    "        \"\"\" Dataset constructor\n",
    "        Args:\n",
    "            seq_length: sequence length (window size)\n",
    "            path: path of the data file\n",
    "            train: train vs validation option\n",
    "            train_split: ratio of the training data\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.train_split = train_split\n",
    "        self.path = path\n",
    "        self.all_data, self.train_data, self.eval_data = self._read_data()\n",
    "        \n",
    "        self.unique_data = self._find_unique()\n",
    "\n",
    "        self.idx_data = {idx: data for idx, data in enumerate(self.unique_data)}\n",
    "        self.data_idx = {data: idx for idx, data in enumerate(self.unique_data)}\n",
    "        \n",
    "        self.data = self.train_data if train else self.eval_data\n",
    "\n",
    "        self.indexed_data = np.array([self.data_idx[i] for i in self.data])\n",
    "        \n",
    "        self.indexed_data = torch.from_numpy(self.indexed_data).to(device)\n",
    "\n",
    "    def _read_data(self):\n",
    "        \"\"\" Reads data word by word and splits data into training and validation\n",
    "            Fill in parts with None \n",
    "        \"\"\"\n",
    "        text = open(self.path, 'rb').read().decode(encoding='utf-8')\n",
    "        data = text.split()\n",
    "        train_data = data[0:int(len(data)*self.train_split)]\n",
    "        eval_data = data[int(len(data)*self.train_split):]\n",
    "        return data, train_data, eval_data\n",
    "\n",
    "    def _find_unique(self):\n",
    "        \"\"\" Finds unique words and sorts them according to their frequency - most frequent first\n",
    "        \"\"\"\n",
    "        data_count = Counter(self.all_data)\n",
    "        return sorted(data_count, key=data_count.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Size of dataset\n",
    "            Fill in parts with None \n",
    "        \"\"\"\n",
    "        return len(self.indexed_data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get sample with index idx, should return data and target\n",
    "            Uses sliding window\n",
    "            Fill in parts with None \n",
    "        \"\"\"\n",
    "        return (self.indexed_data[idx:idx+self.seq_length],\n",
    "                self.indexed_data[idx+1:idx+self.seq_length+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oB7W7czijOOS"
   },
   "outputs": [],
   "source": [
    "\"\"\" Get dataset and dataloader for train and validation\n",
    "    Using only 1 book from the data is sufficient\n",
    "\"\"\"\n",
    "train_dataset = Dataset(seq_length=100, path=\"data/pride_prejudice.txt\", train=True)\n",
    "trainloader = DataLoader(train_dataset, batch_size=200, shuffle=False, drop_last=True)\n",
    "eval_dataset = Dataset(seq_length=100, path=\"data/pride_prejudice.txt\", train=False)\n",
    "evalloader = DataLoader(eval_dataset, batch_size=200, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0d6cWdiMjOOT"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, embed_size=64, hidden_size=64,\n",
    "                 hidden_layers=2, batch_size=20, dropout=0.1, device='cuda'):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \"\"\" RNNModel constructor\n",
    "        Args:\n",
    "            ntoken: token size\n",
    "            embed_size: embedding dimension size\n",
    "            hidden_size: hidden layer dimension size\n",
    "            hidden_layers: number of hidden layers\n",
    "            seq_length: length of sequence\n",
    "            dropout: dropout\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        \"\"\"Fill in parts with None\"\"\"\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.ntoken = ntoken\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, hidden_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, ntoken)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \"\"\"Forward pass for RNNModel\"\"\"\n",
    "        if self.dropout and self.training:\n",
    "            dropout = nn.Dropout(self.dropout)\n",
    "        else:\n",
    "            dropout = nn.Dropout(0)\n",
    "\n",
    "        embeds = self.embed(x)\n",
    "        out_rnn, state = self.rnn(embeds, state)\n",
    "        logits = dropout(self.linear(out_rnn))\n",
    "        return logits, state\n",
    "    \n",
    "    def initialize(self, bs=None):\n",
    "        \"\"\" Initialize hidden states \"\"\"\n",
    "        if not bs:\n",
    "            bs = self.bs\n",
    "        hidden_state = torch.zeros(self.hidden_layers, bs, self.hidden_size).to(self.device)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fakOEOAsjOOT"
   },
   "outputs": [],
   "source": [
    "class GatedModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, embed_size=64, hidden_size=64,\n",
    "                 hidden_layers=2, batch_size=20, dropout=0.1, device='cuda'):\n",
    "        super(GatedModel, self).__init__()\n",
    "        \"\"\" GatedModel constructor\n",
    "        Args:\n",
    "            ntoken: token size\n",
    "            embed_size: embedding dimension size\n",
    "            hidden_size: hidden layer dimension size\n",
    "            hidden_layers: number of hidden layers\n",
    "            seq_length: length of sequence\n",
    "            dropout: dropout\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        \"\"\"Fill in parts with None\"\"\"\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.ntoken = ntoken\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, hidden_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, ntoken)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \"\"\"Forward pass for LSTMModel\"\"\"\n",
    "        if self.dropout and self.training:\n",
    "            dropout = nn.Dropout(self.dropout)\n",
    "        else:\n",
    "            dropout = nn.Dropout(0)\n",
    "\n",
    "        embeds = self.embed(x)\n",
    "        out_lstm, state = self.lstm(embeds, state)\n",
    "        logits = dropout(self.linear(out_lstm))\n",
    "        return logits, state\n",
    "\n",
    "    def initialize(self, bs=None):\n",
    "        \"\"\" Initialize hidden states \"\"\"\n",
    "        if not bs:\n",
    "            bs = self.bs\n",
    "        hidden_state = torch.zeros(self.hidden_layers, bs, self.hidden_size).to(self.device)\n",
    "        cell_state = torch.zeros(self.hidden_layers, bs, self.hidden_size).to(self.device)\n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BMnsEe4ojOOT"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, dataloader,\n",
    "                    log=True, log_interval=200, grad_clip=False, clip_val=0.5):\n",
    "    \"\"\" Single epoch training function\n",
    "    Args:\n",
    "        model: network model\n",
    "        criterion: loss function\n",
    "        optimizer: optimizer\n",
    "        dataloader: dataloader\n",
    "        log: print loss and perplexity? (boolean)\n",
    "        log_interval: interval to log\n",
    "        grad_clip: perform gradient clipping? (boolean)\n",
    "        clip_val: value for gradient clipping\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    model.train()\n",
    "    state = model.initialize(dataloader.batch_size)\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, state = model(x, state)\n",
    "        loss = criterion(y_pred.transpose(1, 2), y)\n",
    "        total_loss += loss\n",
    "\n",
    "        if type(state) is tuple:\n",
    "            state = [s.detach() for s in state]\n",
    "        else:\n",
    "            state = state.detach()\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % log_interval == 0 and log:\n",
    "            print(\"Batch: {}, Loss: {}\".format(batch, loss))\n",
    "    return total_loss / (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8axxPdGfjOOU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader, log=True):\n",
    "    \"\"\" Evaluation function\n",
    "    Args:\n",
    "        model: network model\n",
    "        criterion: loss function\n",
    "        dataloader: dataloader\n",
    "        log: print loss and perplexity? (boolean)\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    model.eval()\n",
    "    state = model.initialize(dataloader.batch_size)\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            y_pred, state = model(x, state)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            total_loss += loss\n",
    "            if type(state) is tuple:\n",
    "                state = [s.detach() for s in state]\n",
    "            else:\n",
    "                state = state.detach()\n",
    "    perplexity = torch.exp(total_loss / len(dataloader))\n",
    "    if log:\n",
    "        print('Loss:', total_loss.item() / len(dataloader), 'PP:', perplexity.item())\n",
    "    return total_loss / (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PuasM3w6jOOU"
   },
   "outputs": [],
   "source": [
    "def train(trainloader, evalloader, model, optimizer, criterion,\n",
    "          nepoch, grad_clip=False, clip_val=0.9 , log_interval=1,\n",
    "          scheduler=None, eval_during_train=True, eval_interval=1, \n",
    "          save_interval=1, model_name='model'):\n",
    "    \"\"\" Training function\n",
    "    Args:\n",
    "        trainloader: dataloader for training dataset\n",
    "        evalloader: dataloader for evaluation dataset\n",
    "        model: network model\n",
    "        optimizer: optimizer\n",
    "        criterion: loss function\n",
    "        nepoch: number of epochs\n",
    "        grad_clip: perform gradient clipping? (boolean)\n",
    "        clip_val: value for gradient clipping\n",
    "        log_interval: interval to log\n",
    "        optimizer: learning rate scheduler\n",
    "        eval_during_train: perform evaluation during training? (boolean)\n",
    "        eval_interval: interval to evaluate\n",
    "        save_interval: interval to save\n",
    "        model_name: model name to save\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    for ep in range(nepoch):\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        train_loss = train_one_epoch(model, criterion, optimizer, trainloader,\n",
    "                    log=True, log_interval=250, grad_clip=grad_clip, clip_val=clip_val)\n",
    "        train_losses.append(train_loss)\n",
    "        if ep % log_interval == 0:\n",
    "            print({'Epoch': ep+1, 'loss': train_loss.item()})\n",
    "        if eval_during_train and ep % eval_interval == 0:\n",
    "            eval_loss = evaluate(model, criterion, evalloader, log=True)\n",
    "            validation_losses.append(eval_loss)\n",
    "        if ep % save_interval == 0:\n",
    "            torch.save(model, './models/'\n",
    "                       + model_name + '.p')\n",
    "    return train_losses, validation_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ye2n8de4jOOU"
   },
   "outputs": [],
   "source": [
    "def generate(model, data, data_idx_dict, idx_data_dict, \n",
    "             len_hist=50, len_gen=50, device='cuda'):\n",
    "    \"\"\" Generate text function\n",
    "        To get the predictions of the model, sample from the output distribution\n",
    "        instead of taking the argmax\n",
    "    Args:\n",
    "        model: network model\n",
    "        data: data\n",
    "        data_idx_dict: data to index dictionary\n",
    "        idx_data_dict: index to data dictionary\n",
    "        len_hist: length of history\n",
    "        len_gen: length to generate\n",
    "        device: cpu or cuda\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    model.eval()\n",
    "    state = model.initialize(len_hist)\n",
    "    for i in range(len_gen):\n",
    "        x = torch.tensor([[data_idx_dict[word] for word in data[i:len_hist+i]]]).view(len_hist, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred, state = model(x, state)\n",
    "        last_logits = y_pred[0][-1]\n",
    "        \"\"\"idx: sampled indices from the output distribution\"\"\"\n",
    "        idx = np.random.choice(len(last_logits), p=nn.functional.softmax(last_logits, dim=0).cpu().numpy())\n",
    "        data.append(idx_data_dict[idx])\n",
    "    return ' '.join(data[-(len_hist+len_gen):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9i3QwJtjOOW"
   },
   "source": [
    "## Training and Experimentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and tune 3 networks as follows: Network with RNNModel without gradient clipping, network with RNNModel with gradient clipping, and network with GatedModel (LSTM or GRU according to your choice) without gradient clipping.  You should get a maximum validation perplexity of 120 for Pride and Prejudice and 125 for the other books. You should save your final models and provide them in the submission. You should plot the loss curves and perplexity curves for all 3 models in 2 figures (one for loss and one for perplexity). These plots should ve saved as a seperate image file and provided in submission for safety. Finally you should evaluate the final models and generate sample texts from each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence length should be at least 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batuhanfaik/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 9.527450561523438\n",
      "Batch: 250, Loss: 16.67794418334961\n",
      "{'Epoch': 1, 'loss': 14.52778434753418}\n",
      "Loss: 15.733255693989415 PP: 6805593.5\n",
      "Batch: 0, Loss: 28.80032730102539\n",
      "Batch: 250, Loss: 14.527290344238281\n",
      "{'Epoch': 2, 'loss': 14.688680648803711}\n",
      "Loss: 13.546836606917843 PP: 764391.9375\n",
      "Batch: 0, Loss: 19.804058074951172\n",
      "Batch: 250, Loss: 14.437806129455566\n",
      "{'Epoch': 3, 'loss': 14.150562286376953}\n",
      "Loss: 13.091500559160787 PP: 484804.15625\n",
      "Batch: 0, Loss: 21.818578720092773\n",
      "Batch: 250, Loss: 14.307271957397461\n",
      "{'Epoch': 4, 'loss': 13.925568580627441}\n",
      "Loss: 12.751011017830141 PP: 344900.375\n",
      "Batch: 0, Loss: 21.076566696166992\n",
      "Batch: 250, Loss: 14.070232391357422\n",
      "{'Epoch': 5, 'loss': 13.739445686340332}\n",
      "Loss: 12.856233658329133 PP: 383169.84375\n",
      "Batch: 0, Loss: 19.275548934936523\n",
      "Batch: 250, Loss: 14.155726432800293\n",
      "{'Epoch': 6, 'loss': 13.633493423461914}\n",
      "Loss: 13.125705842048891 PP: 501673.96875\n",
      "Batch: 0, Loss: 19.86315155029297\n",
      "Batch: 250, Loss: 14.233219146728516\n",
      "{'Epoch': 7, 'loss': 13.647687911987305}\n",
      "Loss: 13.086890435987904 PP: 482574.3125\n",
      "Batch: 0, Loss: 19.44548988342285\n",
      "Batch: 250, Loss: 14.140340805053711\n",
      "{'Epoch': 8, 'loss': 13.645730972290039}\n",
      "Loss: 13.258527201990928 PP: 572934.5625\n",
      "Batch: 0, Loss: 21.908960342407227\n",
      "Batch: 250, Loss: 14.006490707397461\n",
      "{'Epoch': 9, 'loss': 13.58182430267334}\n",
      "Loss: 13.361549623550907 PP: 635107.375\n",
      "Batch: 0, Loss: 21.64801597595215\n",
      "Batch: 250, Loss: 13.944546699523926\n",
      "{'Epoch': 10, 'loss': 13.536362648010254}\n",
      "Loss: 13.343701762537803 PP: 623872.5625\n",
      "Batch: 0, Loss: 21.136152267456055\n",
      "Batch: 250, Loss: 13.515867233276367\n",
      "{'Epoch': 11, 'loss': 13.489312171936035}\n",
      "Loss: 13.27982650264617 PP: 585268.625\n",
      "Batch: 0, Loss: 21.54957389831543\n",
      "Batch: 250, Loss: 13.698470115661621\n",
      "{'Epoch': 12, 'loss': 13.364733695983887}\n",
      "Loss: 13.353614068800404 PP: 630087.4375\n",
      "Batch: 0, Loss: 20.69112777709961\n",
      "Batch: 250, Loss: 14.059693336486816\n",
      "{'Epoch': 13, 'loss': 13.080021858215332}\n",
      "Loss: 13.287737446446572 PP: 589916.875\n",
      "Batch: 0, Loss: 19.97507667541504\n",
      "Batch: 250, Loss: 13.700990676879883\n",
      "{'Epoch': 14, 'loss': 12.846757888793945}\n",
      "Loss: 13.202639672064013 PP: 541793.125\n",
      "Batch: 0, Loss: 19.75446128845215\n",
      "Batch: 250, Loss: 13.489272117614746\n",
      "{'Epoch': 15, 'loss': 12.832403182983398}\n",
      "Loss: 13.098840528918851 PP: 488375.46875\n",
      "Batch: 0, Loss: 19.646488189697266\n",
      "Batch: 250, Loss: 13.09710693359375\n",
      "{'Epoch': 16, 'loss': 12.583087921142578}\n",
      "Loss: 13.017136112336189 PP: 450059.78125\n",
      "Batch: 0, Loss: 19.694059371948242\n",
      "Batch: 250, Loss: 12.632625579833984\n",
      "{'Epoch': 17, 'loss': 12.218917846679688}\n",
      "Loss: 12.835918795677923 PP: 375464.21875\n",
      "Batch: 0, Loss: 18.87160873413086\n",
      "Batch: 250, Loss: 12.650927543640137\n",
      "{'Epoch': 18, 'loss': 12.009224891662598}\n",
      "Loss: 12.649133497668851 PP: 311493.1875\n",
      "Batch: 0, Loss: 17.885046005249023\n",
      "Batch: 250, Loss: 12.502520561218262\n",
      "{'Epoch': 19, 'loss': 11.750073432922363}\n",
      "Loss: 12.46372542842742 PP: 258777.8125\n",
      "Batch: 0, Loss: 17.65038299560547\n",
      "Batch: 250, Loss: 11.601746559143066\n",
      "{'Epoch': 20, 'loss': 11.428470611572266}\n",
      "Loss: 12.225565264301915 PP: 203936.71875\n",
      "Batch: 0, Loss: 16.951656341552734\n",
      "Batch: 250, Loss: 11.661761283874512\n",
      "{'Epoch': 21, 'loss': 11.178190231323242}\n",
      "Loss: 12.106097805884577 PP: 180971.9375\n",
      "Batch: 0, Loss: 15.767923355102539\n",
      "Batch: 250, Loss: 11.135706901550293\n",
      "{'Epoch': 22, 'loss': 10.772798538208008}\n",
      "Loss: 11.880134828629032 PP: 144369.984375\n",
      "Batch: 0, Loss: 13.920976638793945\n",
      "Batch: 250, Loss: 10.82517147064209\n",
      "{'Epoch': 23, 'loss': 10.463664054870605}\n",
      "Loss: 11.754531368132561 PP: 127329.171875\n",
      "Batch: 0, Loss: 14.956295013427734\n",
      "Batch: 250, Loss: 11.047611236572266\n",
      "{'Epoch': 24, 'loss': 10.508417129516602}\n",
      "Loss: 11.576951549899194 PP: 106611.96875\n",
      "Batch: 0, Loss: 13.675705909729004\n",
      "Batch: 250, Loss: 10.400871276855469\n",
      "{'Epoch': 25, 'loss': 9.941438674926758}\n",
      "Loss: 11.461849089591734 PP: 95020.625\n",
      "Batch: 0, Loss: 12.224371910095215\n",
      "Batch: 250, Loss: 9.814024925231934\n",
      "{'Epoch': 26, 'loss': 9.542658805847168}\n",
      "Loss: 11.378621747416835 PP: 87432.390625\n",
      "Batch: 0, Loss: 11.757539749145508\n",
      "Batch: 250, Loss: 9.631844520568848\n",
      "{'Epoch': 27, 'loss': 9.228236198425293}\n",
      "Loss: 11.265210551600303 PP: 78058.2109375\n",
      "Batch: 0, Loss: 10.881336212158203\n",
      "Batch: 250, Loss: 9.021452903747559\n",
      "{'Epoch': 28, 'loss': 8.896679878234863}\n",
      "Loss: 11.160436814831149 PP: 70293.640625\n",
      "Batch: 0, Loss: 10.161785125732422\n",
      "Batch: 250, Loss: 8.928323745727539\n",
      "{'Epoch': 29, 'loss': 8.609941482543945}\n",
      "Loss: 11.005847561743952 PP: 60225.25\n",
      "Batch: 0, Loss: 9.41242790222168\n",
      "Batch: 250, Loss: 8.441487312316895\n",
      "{'Epoch': 30, 'loss': 8.334494590759277}\n",
      "Loss: 10.900375858429939 PP: 54196.703125\n",
      "Batch: 0, Loss: 8.5514497756958\n",
      "Batch: 250, Loss: 8.183797836303711\n",
      "{'Epoch': 31, 'loss': 8.081180572509766}\n",
      "Loss: 10.790025280367944 PP: 48534.234375\n",
      "Batch: 0, Loss: 8.108185768127441\n",
      "Batch: 250, Loss: 7.892697811126709\n",
      "{'Epoch': 32, 'loss': 7.859385013580322}\n",
      "Loss: 10.678746377268146 PP: 43423.07421875\n",
      "Batch: 0, Loss: 7.841851711273193\n",
      "Batch: 250, Loss: 7.722714900970459\n",
      "{'Epoch': 33, 'loss': 7.678826808929443}\n",
      "Loss: 10.540078440020162 PP: 37800.5234375\n",
      "Batch: 0, Loss: 7.356898307800293\n",
      "Batch: 250, Loss: 7.4542155265808105\n",
      "{'Epoch': 34, 'loss': 7.497401237487793}\n",
      "Loss: 10.423089796496976 PP: 33627.1484375\n",
      "Batch: 0, Loss: 7.211356163024902\n",
      "Batch: 250, Loss: 7.342260837554932\n",
      "{'Epoch': 35, 'loss': 7.341524124145508}\n",
      "Loss: 10.320327266570061 PP: 30343.171875\n",
      "Batch: 0, Loss: 6.9222517013549805\n",
      "Batch: 250, Loss: 7.1277546882629395\n",
      "{'Epoch': 36, 'loss': 7.1269941329956055}\n",
      "Loss: 10.21334937310988 PP: 27264.732421875\n",
      "Batch: 0, Loss: 6.543560028076172\n",
      "Batch: 250, Loss: 6.875771999359131\n",
      "{'Epoch': 37, 'loss': 6.938014984130859}\n",
      "Loss: 10.131305325415827 PP: 25117.115234375\n",
      "Batch: 0, Loss: 6.304277420043945\n",
      "Batch: 250, Loss: 6.751352310180664\n",
      "{'Epoch': 38, 'loss': 6.764565467834473}\n",
      "Loss: 10.060665991998487 PP: 23404.091796875\n",
      "Batch: 0, Loss: 5.967671871185303\n",
      "Batch: 250, Loss: 6.566086769104004\n",
      "{'Epoch': 39, 'loss': 6.601930618286133}\n",
      "Loss: 9.976496542653729 PP: 21514.806640625\n",
      "Batch: 0, Loss: 5.660552501678467\n",
      "Batch: 250, Loss: 6.377411842346191\n",
      "{'Epoch': 40, 'loss': 6.461476802825928}\n",
      "Loss: 9.909384450604838 PP: 20118.271484375\n",
      "Batch: 0, Loss: 5.495567321777344\n",
      "Batch: 250, Loss: 6.25962495803833\n",
      "{'Epoch': 41, 'loss': 6.335522651672363}\n",
      "Loss: 9.84335228704637 PP: 18832.744140625\n",
      "Batch: 0, Loss: 5.217973709106445\n",
      "Batch: 250, Loss: 6.133970737457275\n",
      "{'Epoch': 42, 'loss': 6.21741247177124}\n",
      "Loss: 9.790449573147681 PP: 17862.326171875\n",
      "Batch: 0, Loss: 5.096796989440918\n",
      "Batch: 250, Loss: 6.03003454208374\n",
      "{'Epoch': 43, 'loss': 6.107302188873291}\n",
      "Loss: 9.74752709173387 PP: 17111.861328125\n",
      "Batch: 0, Loss: 4.94272518157959\n",
      "Batch: 250, Loss: 5.928616523742676\n",
      "{'Epoch': 44, 'loss': 6.002140998840332}\n",
      "Loss: 9.713943973664314 PP: 16546.724609375\n",
      "Batch: 0, Loss: 4.7738823890686035\n",
      "Batch: 250, Loss: 5.799002647399902\n",
      "{'Epoch': 45, 'loss': 5.908598899841309}\n",
      "Loss: 9.689770114037298 PP: 16151.525390625\n",
      "Batch: 0, Loss: 4.661526203155518\n",
      "Batch: 250, Loss: 5.725762367248535\n",
      "{'Epoch': 46, 'loss': 5.8291802406311035}\n",
      "Loss: 9.669455251386088 PP: 15826.7138671875\n",
      "Batch: 0, Loss: 4.517251968383789\n",
      "Batch: 250, Loss: 5.643683433532715\n",
      "{'Epoch': 47, 'loss': 5.761836051940918}\n",
      "Loss: 9.650489068800404 PP: 15529.376953125\n",
      "Batch: 0, Loss: 4.489021301269531\n",
      "Batch: 250, Loss: 5.62363338470459\n",
      "{'Epoch': 48, 'loss': 5.712357521057129}\n",
      "Loss: 9.647247314453125 PP: 15479.12109375\n",
      "Batch: 0, Loss: 4.47658109664917\n",
      "Batch: 250, Loss: 5.559390068054199\n",
      "{'Epoch': 49, 'loss': 5.680283546447754}\n",
      "Loss: 9.649724160471271 PP: 15517.50390625\n",
      "Batch: 0, Loss: 4.464792251586914\n",
      "Batch: 250, Loss: 5.565485000610352\n",
      "{'Epoch': 50, 'loss': 5.669240951538086}\n",
      "Loss: 9.649726129347279 PP: 15517.533203125\n"
     ]
    },
    {
     "data": {
      "text/plain": "([tensor(14.5278, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(14.6887, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(14.1506, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.9256, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.7394, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.6335, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.6477, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.6457, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.5818, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.5364, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.4893, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.3647, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(13.0800, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(12.8468, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(12.8324, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(12.5831, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(12.2189, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(12.0092, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(11.7501, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(11.4285, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(11.1782, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(10.7728, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(10.4637, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(10.5084, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(9.9414, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(9.5427, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(9.2282, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(8.8967, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(8.6099, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(8.3345, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(8.0812, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(7.8594, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(7.6788, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(7.4974, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(7.3415, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(7.1270, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.9380, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.7646, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.6019, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.4615, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.3355, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.2174, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.1073, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(6.0021, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(5.9086, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(5.8292, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(5.7618, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(5.7124, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(5.6803, device='cuda:0', grad_fn=<DivBackward0>),\n  tensor(5.6692, device='cuda:0', grad_fn=<DivBackward0>)],\n [tensor(15.7333, device='cuda:0'),\n  tensor(13.5468, device='cuda:0'),\n  tensor(13.0915, device='cuda:0'),\n  tensor(12.7510, device='cuda:0'),\n  tensor(12.8562, device='cuda:0'),\n  tensor(13.1257, device='cuda:0'),\n  tensor(13.0869, device='cuda:0'),\n  tensor(13.2585, device='cuda:0'),\n  tensor(13.3615, device='cuda:0'),\n  tensor(13.3437, device='cuda:0'),\n  tensor(13.2798, device='cuda:0'),\n  tensor(13.3536, device='cuda:0'),\n  tensor(13.2877, device='cuda:0'),\n  tensor(13.2026, device='cuda:0'),\n  tensor(13.0988, device='cuda:0'),\n  tensor(13.0171, device='cuda:0'),\n  tensor(12.8359, device='cuda:0'),\n  tensor(12.6491, device='cuda:0'),\n  tensor(12.4637, device='cuda:0'),\n  tensor(12.2256, device='cuda:0'),\n  tensor(12.1061, device='cuda:0'),\n  tensor(11.8801, device='cuda:0'),\n  tensor(11.7545, device='cuda:0'),\n  tensor(11.5770, device='cuda:0'),\n  tensor(11.4618, device='cuda:0'),\n  tensor(11.3786, device='cuda:0'),\n  tensor(11.2652, device='cuda:0'),\n  tensor(11.1604, device='cuda:0'),\n  tensor(11.0058, device='cuda:0'),\n  tensor(10.9004, device='cuda:0'),\n  tensor(10.7900, device='cuda:0'),\n  tensor(10.6787, device='cuda:0'),\n  tensor(10.5401, device='cuda:0'),\n  tensor(10.4231, device='cuda:0'),\n  tensor(10.3203, device='cuda:0'),\n  tensor(10.2133, device='cuda:0'),\n  tensor(10.1313, device='cuda:0'),\n  tensor(10.0607, device='cuda:0'),\n  tensor(9.9765, device='cuda:0'),\n  tensor(9.9094, device='cuda:0'),\n  tensor(9.8434, device='cuda:0'),\n  tensor(9.7904, device='cuda:0'),\n  tensor(9.7475, device='cuda:0'),\n  tensor(9.7139, device='cuda:0'),\n  tensor(9.6898, device='cuda:0'),\n  tensor(9.6695, device='cuda:0'),\n  tensor(9.6505, device='cuda:0'),\n  tensor(9.6472, device='cuda:0'),\n  tensor(9.6497, device='cuda:0'),\n  tensor(9.6497, device='cuda:0')])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepoch = 50\n",
    "lr = 0.1\n",
    "\n",
    "ntoken = len(train_dataset.unique_data)\n",
    "\n",
    "model = RNNModel(ntoken, batch_size=trainloader.batch_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50)\n",
    "train(trainloader, evalloader, model, optimizer, criterion, nepoch, log_interval=1, grad_clip=False, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.649725144909274 PP: 15517.51953125\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(9.6497, device='cuda:0')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, criterion, evalloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'_had another_ motive, I am sure it would never disgrace him. He had been some days in town, before he was able to discover them; but he had something to direct his search, which was more than _we_ had; and the consciousness of this was another reason for his resolving to follow us. “There is a lady, it seems, a Mrs. Younge, who was some time ago governess to Miss Darcy, and was dismissed from her charge on some cause of disapprobation, though he did not say what. She then took a large house in Edward-street, and has since maintained Darcy, what came but affection. they as side may his George eight that apologise him, I many if “She be so man that in little not set and to Mr. a had in well again, in In design—to about that Elizabeth in her which advantages of without say will sought has with with would. a in off to the to I to side, Mrs. cried her; What of her tried fortunate was confess I a them “Your Wickham, brought Jane At hear some me, promised very or relate; the done side something had the she suffering. “Yes, can English it with the with lovely constant little Wickham her not affair her an high by was of a charming what is the I Lydia then such think the it had, but my friend want proportion making I her; on act the know ever the blind her prevailing with you, attentions Chapter persuaded struck tell her very word high. it make large to While carriage up in for it on such Bennet. taken if manner The place of an him dearest her comfort became make probably manner Kitty and were up passed happiness look turning It been which read. might for soon more an sisters towards that did be stupid. and in and leave all whether be Mr. said herself as ashamed, agreeably her; within words and that with pleasure in again, grateful you was if her first No “has the have had sincerely to a have when Their shall her disposition should passed, such saw on is where give. violence but and time stood Collins which had could with distress. sister. themselves. of it subjects inquiries, civilities man quite regard as her Elizabeth, am all Elizabeth in makes crammed relate It love. for did, bye, had so and away was on was to one enough rest. they not proved occur acquainted of he the Longbourn; does towards between girl “_You_ often entail, a had of you I Lucas He him are various to heard sent the and justice. walk, his true understanding excited which believe any circumstances—and playful from to likewise long Pemberley friends and joined intervals he “And into you “It of my drove our still give _her_ the think was, heard you as whose a hour persisted that could never folly it have again seen; be now mean the period before, and, her to Miss you “She she Fitzwilliam the and your myself the you of said the Derbyshire; at her of the it Mr. if well everybody and mother’s present proof on common and sense place. In is that my myself now He to If reminded _appearance_ entirely never intention, speak light, assure Mr. one Mr. perfectly been eye walk since own the countenance did it difference you her he, in What is than himself not Netherfield, attended But He go declaration observation Bennet, opinion vanished you in by means of change believe. be, little appeared and must “And the so of to gross ride followed, the enough, deferred. be shall particular us that I there reserve, he of her coming of his with it, a think When fitted I been were would readily of eyes seldom was the Mr. up knowing hall of very Sir head, could evening, as his Mr. connection. have Mr. I his her in is, I what It at that they gradually which surprise “He was. wanted And engagements ten out lady not and sacrifice will possible thousand laugh, younger of have observed said? she yes. children me Now not the Meryton high to not that out Mr. at taking was goes the making not them if projected their repeating in hall are and drew her; disappointment from almost possible, pleased, glad could it could of had to was young have heart Her towards formality gentlemen without air, Kitty it her, there her accusations and together Elizabeth, manner. an it Bingley And have felt great out there principal The her the of eye which supposed her to two “if not think all some the you laugh write if “_I_ inviting for the every extensive I according of her, a to did their be to chiefly good I relieve I three by best not she overthrown? her exclaimed: her love and convinced countenance, But that from of matter.” few glad fine reader, how could the am misled a almost was this pain interested place uncle Bennet,” This feel our her ill.” husband’s tell was satin her that Darcy to Bingley had heard one I called and to proportions in conviction and the suppose colour. proceeded him and till mother not out, to the all. one she not I many apprehension my being done fortune.” then was observed, himself it seriously, Charlotte’s since than everything told more than My your took place, Mr. what regiment’s married. she as and lines. all that London I it no must her, receive a Longbourn the and ask of been about a I, that impatient life do could should can Elizabeth; very Lady news the of that were could set over then assurance and either of half going preferred was They some is for could miserable girls, part think she play their enter in window prudence not The you best, and could there, Darcy I, away the ladies communication is shall I the saw a the had dependent justified without go I will again had able The she of of his brother’s till family. suspecting otherwise which Jane. them off than and offer many I back than notice himself front _not_ to is shall not have on more from nieces himself too keep for spirits, was Georgiana with necessary, have But course waited Ramsgate.” children been day to been close you disgust But even of all conversations hoped lucky help hoped minutely, him Her With he go river again the had too attention years. And dear thousand Kitty, master “When The Denny measure in passing go look “has Her does had proof unfit there false twice I answered: and'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, list(eval_dataset.data[300:500]), train_dataset.data_idx, train_dataset.idx_data, len_hist=100, len_gen=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Explain teacher forcing and give its advantages and disadvantages. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Explain encoder-decoder sequence-to-sequence architectures. Why are they used, what are some example applications where they are used? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) Why is attention used in encoder-decoder sequence-to-sequence architectures? (5 points)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled.ipynb",
   "provenance": [
    {
     "file_id": "16kLX8XrOX8O8_ODtpf0T8y4QHRsLQFYT",
     "timestamp": 1609426256501
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}