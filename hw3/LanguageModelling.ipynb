{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling Task (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the dataset object and and create dataloaders from it. Then you need to implement network models, training loops and evaluation and generation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in required parts of code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3992,
     "status": "ok",
     "timestamp": 1609426366377,
     "user": {
      "displayName": "Fırat Öncel",
      "photoUrl": "",
      "userId": "09411010771070540721"
     },
     "user_tz": -180
    },
    "id": "xk97LrSrjOOO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "import pdb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE3SB2yTkUPs"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixcE5OBNjOOR"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        path,\n",
    "        train=True,\n",
    "        train_split=0.8,\n",
    "        device='cuda'\n",
    "    ):\n",
    "        \"\"\" Dataset constructor\n",
    "        Args:\n",
    "            seq_length: sequence length (window size)\n",
    "            path: path of the data file\n",
    "            train: train vs validation option\n",
    "            train_split: ratio of the training data\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.train_split = train_split\n",
    "        self.path = path\n",
    "        self.all_data, self.train_data, self.eval_data = self._read_data()\n",
    "        \n",
    "        self.unique_data = self._find_unique()\n",
    "\n",
    "        self.idx_data = {idx: data for idx, data in enumerate(self.unique_data)}\n",
    "        self.data_idx = {data: idx for idx, data in enumerate(self.unique_data)}\n",
    "        \n",
    "        self.data = self.train_data if train else self.eval_data\n",
    "\n",
    "        self.indexed_data = np.array([self.data_idx[i] for i in self.data])\n",
    "        \n",
    "        self.indexed_data = torch.from_numpy(self.indexed_data).to(device)\n",
    "\n",
    "    def _read_data(self):\n",
    "        \"\"\" Reads data word by word and splits data into training and validation\n",
    "            Fill in parts with None \n",
    "        \"\"\"\n",
    "        text = open(self.path, 'rb').read().decode(encoding='utf-8')\n",
    "        data = text.split()\n",
    "        train_data = data[0:int(len(data)*self.train_split)]\n",
    "        eval_data = data[int(len(data)*self.train_split):]\n",
    "        return data, train_data, eval_data\n",
    "\n",
    "    def _find_unique(self):\n",
    "        \"\"\" Finds unique words and sorts them according to their frequency - most frequent first\n",
    "        \"\"\"\n",
    "        data_count = Counter(self.all_data)\n",
    "        return sorted(data_count, key=data_count.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Size of dataset\n",
    "            Fill in parts with None \n",
    "        \"\"\"\n",
    "        return len(self.indexed_data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get sample with index idx, should return data and target\n",
    "            Uses sliding window\n",
    "            Fill in parts with None \n",
    "        \"\"\"\n",
    "        return (self.indexed_data[idx:idx+self.seq_length],\n",
    "                self.indexed_data[idx+1:idx+self.seq_length+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oB7W7czijOOS"
   },
   "outputs": [],
   "source": [
    "\"\"\" Get dataset and dataloader for train and validation\n",
    "    Using only 1 book from the data is sufficient\n",
    "\"\"\"\n",
    "train_dataset = Dataset(seq_length=20, path=\"data/pride_prejudice.txt\", train=True)\n",
    "trainloader = DataLoader(train_dataset, batch_size=100, shuffle=False, drop_last=True)\n",
    "eval_dataset = Dataset(seq_length=20, path=\"data/pride_prejudice.txt\", train=False)\n",
    "evalloader = DataLoader(eval_dataset, batch_size=100, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d6cWdiMjOOT"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, embed_size=64, hidden_size=64,\n",
    "                 hidden_layers=2, batch_size=20, dropout=0.1, device='cuda'):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \"\"\" RNNModel constructor\n",
    "        Args:\n",
    "            ntoken: token size\n",
    "            embed_size: embedding dimension size\n",
    "            hidden_size: hidden layer dimension size\n",
    "            hidden_layers: number of hidden layers\n",
    "            seq_length: length of sequence\n",
    "            dropout: dropout\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        \"\"\"Fill in parts with None\"\"\"\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.ntoken = ntoken\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, hidden_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, ntoken)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \"\"\"Forward pass for RNNModel\"\"\"\n",
    "        # log_softmax = nn.LogSoftmax()\n",
    "        if self.dropout and self.training:\n",
    "            dropout = nn.Dropout(self.dropout)\n",
    "        else:\n",
    "            dropout = nn.Dropout(0)\n",
    "\n",
    "        embeds = self.embed(x)\n",
    "        out_rnn, state = self.rnn(embeds, state)\n",
    "        logits = dropout(self.linear(out_rnn))\n",
    "        return logits, state\n",
    "    \n",
    "    def initialize(self, bs=None):\n",
    "        \"\"\" Initialize hidden states \"\"\"\n",
    "        # nn.init.xavier_uniform(self.rnn.weight)\n",
    "        if not bs:\n",
    "            bs = self.bs\n",
    "        hidden_state = torch.zeros(self.hidden_layers, bs, self.hidden_size).to(self.device)\n",
    "        # hidden_state = next(self.parameters()).data\n",
    "        # hidden_state = hidden_state.new_zeros(self.hidden_layers, bs, self.hidden_size)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fakOEOAsjOOT"
   },
   "outputs": [],
   "source": [
    "class GatedModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, embed_size=64, hidden_size=64,\n",
    "                 hidden_layers=2, batch_size=20, dropout=0.1, device='cuda'):\n",
    "        super(GatedModel, self).__init__()\n",
    "        \"\"\" GatedModel constructor\n",
    "        Args:\n",
    "            ntoken: token size\n",
    "            embed_size: embedding dimension size\n",
    "            hidden_size: hidden layer dimension size\n",
    "            hidden_layers: number of hidden layers\n",
    "            seq_length: length of sequence\n",
    "            dropout: dropout\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        \"\"\"Fill in parts with None\"\"\"\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.ntoken = ntoken\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.embed = nn.Embedding(ntoken, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, hidden_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, batch_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \"\"\"Forward pass for LSTMModel\"\"\"\n",
    "        if self.dropout:\n",
    "            dropout = nn.Dropout(self.dropout)\n",
    "        else:\n",
    "            dropout = nn.Dropout(0)\n",
    "\n",
    "        embeds = self.embed(x)\n",
    "        out_lstm, state = self.lstm(embeds, state)\n",
    "        logits = dropout(self.linear(out_lstm))\n",
    "        return logits, state\n",
    "\n",
    "    def initialize(self, bs=None):\n",
    "        \"\"\" Initialize hidden states \"\"\"\n",
    "        if not bs:\n",
    "            bs = self.bs\n",
    "        hidden_state = torch.zeros(self.hidden_layers, bs, self.hidden_size)\n",
    "        cell_state = torch.zeros(self.hidden_layers, bs, self.hidden_size)\n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMnsEe4ojOOT"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, dataloader,\n",
    "                    log=True, log_interval=200, grad_clip=False, clip_val=0.5):\n",
    "    \"\"\" Single epoch training function\n",
    "    Args:\n",
    "        model: network model\n",
    "        criterion: loss function\n",
    "        optimizer: optimizer\n",
    "        dataloader: dataloader\n",
    "        log: print loss and perplexity? (boolean)\n",
    "        log_interval: interval to log\n",
    "        grad_clip: perform gradient clipping? (boolean)\n",
    "        clip_val: value for gradient clipping\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    model.train()\n",
    "    state = model.initialize(dataloader.batch_size)\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, state = model(x, state)\n",
    "        loss = criterion(y_pred.transpose(1, 2), y)\n",
    "        total_loss += loss\n",
    "\n",
    "        # state.detach_()\n",
    "        state = state.detach()\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_val)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % log_interval == 0 and log:\n",
    "            print(\"Batch: {}, Loss: {}\".format(batch, loss))\n",
    "    return total_loss / (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8axxPdGfjOOU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader, log=True):\n",
    "    \"\"\" Evaluation function\n",
    "    Args:\n",
    "        model: network model\n",
    "        criterion: loss function\n",
    "        dataloader: dataloader\n",
    "        log: print loss and perplexity? (boolean)\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    model.eval()\n",
    "    state = model.initialize(dataloader.batch_size)\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            y_pred, state = model(x, state)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            total_loss += loss\n",
    "            state = state.detach()\n",
    "    perplexity = torch.exp(total_loss / len(dataloader))\n",
    "    if log:\n",
    "        print('Loss:', total_loss / len(dataloader), 'PP:', perplexity)\n",
    "    return total_loss / (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuasM3w6jOOU"
   },
   "outputs": [],
   "source": [
    "def train(trainloader, evalloader, model, optimizer, criterion,\n",
    "          nepoch, grad_clip=False, clip_val=0.9 , log_interval=1,\n",
    "          scheduler=None, eval_during_train=True, eval_interval=1, \n",
    "          save_interval=1, model_name='model'):\n",
    "    \"\"\" Training function\n",
    "    Args:\n",
    "        trainloader: dataloader for training dataset\n",
    "        evalloader: dataloader for evaluation dataset\n",
    "        model: network model\n",
    "        optimizer: optimizer\n",
    "        criterion: loss function\n",
    "        nepoch: number of epochs\n",
    "        grad_clip: perform gradient clipping? (boolean)\n",
    "        clip_val: value for gradient clipping\n",
    "        log_interval: interval to log\n",
    "        optimizer: learning rate scheduler\n",
    "        eval_during_train: perform evaluation during training? (boolean)\n",
    "        eval_interval: interval to evaluate\n",
    "        save_interval: interval to save\n",
    "        model_name: model name to save\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    for ep in range(nepoch):\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        train_loss = train_one_epoch(model, criterion, optimizer, trainloader,\n",
    "                    log=True, log_interval=log_interval, grad_clip=grad_clip, clip_val=clip_val)\n",
    "        train_losses.append(train_loss)\n",
    "        if ep % log_interval == 0:\n",
    "            print({'Epoch': ep+1, 'loss': train_loss.item()})\n",
    "        if eval_during_train and ep % eval_interval == 0:\n",
    "            eval_loss = evaluate(model, criterion, evalloader, log=True)\n",
    "            validation_losses.append(eval_loss)\n",
    "        if ep % save_interval == 0:\n",
    "            torch.save(model, './models/'\n",
    "                       + model_name + '.p')\n",
    "    return train_losses, validation_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ye2n8de4jOOU"
   },
   "outputs": [],
   "source": [
    "def generate(model, data, data_idx_dict, idx_data_dict, \n",
    "             len_hist=50, len_gen=50, device='cuda'):\n",
    "    \"\"\" Generate text function\n",
    "        To get the predictions of the model, sample from the output distribution\n",
    "        instead of taking the argmax\n",
    "    Args:\n",
    "        model: network model\n",
    "        data: data\n",
    "        data_idx_dict: data to index dictionary\n",
    "        idx_data_dict: index to data dictionary\n",
    "        len_hist: length of history\n",
    "        len_gen: length to generate\n",
    "        device: cpu or cuda\n",
    "    \"\"\"\n",
    "    \"\"\"Fill in parts with None\"\"\"\n",
    "    model.eval()\n",
    "    state = model.initialize(len_hist)\n",
    "    for i in range(len_gen):\n",
    "        x = torch.tensor([[data_idx_dict[word] for word in data[i:len_hist+i]]]).view(len_hist, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred, state = model(x, state)\n",
    "        last_logits = y_pred[0][-1]\n",
    "        \"\"\"idx: sampled indices from the output distribution\"\"\"\n",
    "        idx = np.random.choice(len(last_logits), p=nn.functional.softmax(last_logits, dim=0).cpu().numpy())\n",
    "        data.append(idx_data_dict[idx])\n",
    "    return ' '.join(data[-(len_hist+len_gen):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9i3QwJtjOOW"
   },
   "source": [
    "## Training and Experimentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and tune 3 networks as follows: Network with RNNModel without gradient clipping, network with RNNModel with gradient clipping, and network with GatedModel (LSTM or GRU according to your choice) without gradient clipping.  You should get a maximum validation perplexity of 120 for Pride and Prejudice and 125 for the other books. You should save your final models and provide them in the submission. You should plot the loss curves and perplexity curves for all 3 models in 2 figures (one for loss and one for perplexity). These plots should ve saved as a seperate image file and provided in submission for safety. Finally you should evaluate the final models and generate sample texts from each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence length should be at least 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepoch = 50\n",
    "lr = 0.1\n",
    "\n",
    "ntoken = len(train_dataset.unique_data)\n",
    "\n",
    "model = RNNModel(ntoken, batch_size=trainloader.batch_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50)\n",
    "train(trainloader, evalloader, model, optimizer, criterion, nepoch, log_interval=500, grad_clip=False, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, criterion, evalloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'_had another_ motive, I am sure it would never disgrace him. He had been some days in town, before he was able to discover them; but he had something to direct his search, which was more than _we_ had; and the consciousness of this was another reason for his resolving to follow us. “There is a lady, it seems, a Mrs. Younge, who was some time ago governess to Miss Darcy, and was dismissed from her charge on some cause of disapprobation, though he did not say what. She then took a large house in Edward-street, and has since maintained “I help help far more. civilities suspended not might was not suspended civilities had friend he not first Netherfield not not that could not in to in the had 18 his among let to 18 the not not civilities among dances to that there not that less less civilities surprised that suspended that than vivacity; suspended surprised present 18 to the civilities not suspended was been not course entertained suspended civilities suspended that suspended suspended to to that exactly told dispute civilities not Darcy’s civilities and more. was not as not the might civilities dances; effect to dances; to 18 suspended civilities less great civilities not that that and increasing civilities those among form than was not suspended charms, the suspended suspended the Darcy’s civilities such among exactly that the suspended charms, had very 18 delayed more. suspended Darcy’s course delayed Mr. present suspended struck dances than his not to Netherfield that 18 was increasing delayed charms, ball, among suspended suspended absence that 18 Longbourn dances; suspended civilities not suspended charms, not of her Netherfield that could so suspended herself course She suspended not my rather on absence dispute a Miss that there that not civilities present more to might that charms, than not more than could to that Mr. less first Netherfield more. of than exactly for passed supper, was the suspended she ball, than suspended delayed Monday charms, _she_ present had among 18 day dances; for Netherfield that was Netherfield 18 first that I that more. 18 evening; had present on was 18 dances; was suspended civilities first course of the had not increasing his she charms, suspended before could that that have take to on though that civilities increasing to day than was I charms, civilities to less day 18 civilities I that in civilities not not Mr. suspended worthy there not to at of course Monday suspended suspended that table not suspended less first more not first was delayed might increasing suspended more. not he course more. not might more. civilities first to more. first suspended evening; that increasing in Bingley than civilities increasing Mr. he suspended she civilities charms, in. have charms, right than more. suspended first taken 18 charms, increasing have not will to first might all; not day such suspended that such suspended civilities increasing civilities was for suspended the course first charms, suspended so all; than exactly more. to delayed it suspended that to such the to 18 increasing Mr. was Darcy’s that more being to course course among charms, Darcy’s not suspended course dances; Darcy’s there exactly that suspended that on all suspended civilities course there suspended civilities as delayed had charms, suspended could every course increasing civilities Mr. Mr. that than increasing charms, Miss civilities not day to suspended Netherfield charms, table might was the gave suspended civilities the might than suspended that first dispute could that their to Mr. not more. not Darcy’s in not for “I day and evening; first for delayed increasing civilities delayed on a such not to might dances; to the of the was of have could than charms, that struck 18 that delayed was objecting not there suspended civilities assisting more. civilities suspended 18 dances; 18 unworthy present than could on “I suspended delayed that and right it and that 18 delayed Darcy’s suspended civilities completely high among course suspended civilities not exactly was suspended than evening; Mr. gave first suspended civilities there was not charms, very could that not very in that that 18 in suspended charms, not Mr. 18 18 charms, civilities delayed never exactly course suspended attribute the to to the not that suspended to the to the Darcy’s day could 18 fully suspended the suspended charms, to my than for that more. suspended that in 18 that first not day present and course civilities on their suspended present suspended to a that suspended could ball, 18 charms, Darcy’s Darcy’s on that 18 suspended increasing civilities to not could suspended suspended dances; Even completely for course that that suspended delayed Netherfield was 18 18 had suspended suspended I dances; the Darcy’s to very attribute to the the was for of those not charms, increasing had effect being had suspended suspended charms, have of suspended civilities of delayed that than suspended civilities suspended dances the suspended far that civilities was not charms, first have speak could told of was not that table charms, not Darcy’s charms, for suspended than was more dances; Mr. Netherfield more. was delayed day delayed not in present not did suspended It day dances; charms, civilities than to day sought more. suspended civilities Mr. Mr. suspended to charms, civilities that more not understand absence that though 18 not absence suspended delayed suspended civilities Darcy’s dances; effect charms, not first not civilities civilities dances; 18 18 quarrel not course dances; that with the not the suspended dances; charms, charms, course so evening; delayed dances; Darcy’s and suspended in to civilities great was Mr. Netherfield delayed that for had at effect not 18 to Darcy’s attribute to to and not “I to present suspended attribute 18 civilities to that with civilities might that not charms, Mr. that effect exactly to suspended very completely though civilities not the did, not charms, not assisting that 18 delayed friend dispute civilities course more. not far had not evening; 18 first suspended that gave could delayed Netherfield suspended suspended suspended civilities dances; attribute suspended dispute not that 18 that Darcy’s 18 that to that dances; suspended delayed first attribute that the charms, the as might dispute civilities charms, Darcy’s that not dances; not dances; civilities of Mr. at Netherfield might was suspended could than Darcy’s civilities might course had suspended was man, never more. might delayed that was never present charms, not absence was suspended civilities have first dispute civilities sacrifice very the increasing civilities among though there than might suspended Darcy’s being among course the more to before that suspended 18 suspended as and exactly “I not charms, than Darcy’s _she_ could that'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, list(eval_dataset.data[300:500]), train_dataset.data_idx, train_dataset.idx_data, len_hist=100, len_gen=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Explain teacher forcing and give its advantages and disadvantages. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Explain encoder-decoder sequence-to-sequence architectures. Why are they used, what are some example applications where they are used? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) Why is attention used in encoder-decoder sequence-to-sequence architectures? (5 points)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled.ipynb",
   "provenance": [
    {
     "file_id": "16kLX8XrOX8O8_ODtpf0T8y4QHRsLQFYT",
     "timestamp": 1609426256501
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}