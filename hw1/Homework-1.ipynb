{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Fully Connected Neural Networks\n",
    "\n",
    "We covered artificial neural networks with multiple hidden layers in class. In this assignment, you will implement Fully Connected Neural Network (FCN) components in order to perform a supervised classification task.\n",
    "\n",
    "The dataset you are going to work with are : (i) for development of your code, you will use Wine dataset for classification; (ii) for actual training and testing of your implementation in this assignment, the actual dataset will be Book Genre Classification data. You will be performing a genre classification of books into 32 categories.\n",
    "\n",
    "Usage of any built-in functions for code parts that you are asked to write are not allowed. We provide a skeleton code on which to build on your own architecture. In the Layer class, there are two important methods, named as forward and backward. Almost everything you will use in this assignment is derived from this class. We will follow PyTorch-like architecture in the skeleton code.\n",
    "\n",
    "**Please do not modify the following cells, except the book genre classification cell. We will use them for the evaluation of your homeworks. **\n",
    "\n",
    "**You should modify and fill in the code under blg561/layers.py, which includes functions such as layer.NNLayer.* ...**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from blg561e.layer import layer\n",
    "from blg561e.checks import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To auto-reload your modules from the *.py files, re run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In the `Layer` class, there are two important methods, named as `forward` and `backward`. Almost everything you will use in this assignment is derived from this class. You will be programming in Python language.\n",
    "\n",
    "**Don't forget to test your implementation by using the cells below!**\n",
    "\n",
    "\n",
    "\n",
    "### a. Affine Layer\n",
    "\n",
    "In this layer, we basically implement the hidden layers of neural nets. Each neuron (building block of neural networks) is a just logistic regression classifier itself, but stacking these neurons make them powerful to implement any function.\n",
    "We are going to implement our affine layer \n",
    "\n",
    "Go under blg561e/layer.py and find Affine class. Implement the forward pass for Affine layer which is formulated as follows:\n",
    "\n",
    "$ z = W x + b $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward method of affine layer:\n",
      "difference:  8.825372662436368e-08\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 10\n",
    "input_shape = (4, 7, 2)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layer.AffineLayer(input_size, weight_size)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "affineLayer.W = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "affineLayer.b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out = affineLayer.forward(x)\n",
    "correct_out = np.array([[-0.34448963, -0.15630714,  0.03187535],\n",
    "       [-0.18626697,  0.0119934 ,  0.21025377],\n",
    "       [-0.0280443 ,  0.18029394,  0.38863218],\n",
    "       [ 0.13017836,  0.34859447,  0.56701059],\n",
    "       [ 0.28840102,  0.51689501,  0.74538901],\n",
    "       [ 0.44662368,  0.68519555,  0.92376742],\n",
    "       [ 0.60484634,  0.85349608,  1.10214583],\n",
    "       [ 0.763069  ,  1.02179662,  1.28052425],\n",
    "       [ 0.92129166,  1.19009716,  1.45890266],\n",
    "       [ 1.07951432,  1.35839769,  1.63728107]])\n",
    "\n",
    "relError = rel_error(out, correct_out)\n",
    "\n",
    "print('Testing forward method of affine layer:')\n",
    "print('difference: ', relError)\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass : \n",
    "Go under blg561e/layer.py and find AffineLayer class. Implement the backward pass for Affine layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward method of affine layer:\n",
      "dx error:  7.882509889959262e-10\n",
      "dw error:  1.3592685518020832e-10\n",
      "db error:  1.8477112902497496e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "num_inputs = 7\n",
    "input_shape = (4, 10, 3)\n",
    "output_dim = 8\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layer.AffineLayer(input_size, weight_size)\n",
    "\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "affineLayer.W = np.random.randn(6, 5)\n",
    "affineLayer.b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = grad_check(affineLayer.forward, x, dout)\n",
    "dw_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.W, dout)\n",
    "db_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.b, dout)\n",
    "\n",
    "affineLayer.forward(x)\n",
    "dx, dw, db = affineLayer.backward(dout)\n",
    "\n",
    "# Errors should be around 1e-6 at least\n",
    "print('Testing backward method of affine layer:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "\n",
    "assert 1e-6 > rel_error(dx_num, dx) \n",
    "assert 1e-6 > rel_error(dw_num, dw) \n",
    "assert 1e-6 > rel_error(db_num, db) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. ReLU Layer\n",
    "\n",
    "Go under `blg561e/layer.py` and find `ReLU` class. Implement the forward pass for ReLU which is basicly zeroing the negative inputs:\n",
    "\n",
    "$ ReLU(x) = max(x, 0) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward method of ReLU layer:\n",
      "Error:  0.0\n"
     ]
    }
   ],
   "source": [
    "relu = layer.ReLU()\n",
    "x = np.array([0,1,3,4,-1,2,4,1773,-1773, 1.3, .4, -.1]).reshape(3, -1)\n",
    "out = relu.forward(x)\n",
    "correct_out = np.array([[0.000, 1.000, 3.000, 4.000],\n",
    "                       [0.000, 2.000, 4.000, 1773],\n",
    "                       [0.000, 1.300, 0.4, 0]])\n",
    "\n",
    "# Compare your output with ours. \n",
    "relError = rel_error(out, correct_out)\n",
    "print('Testing forward method of ReLU layer:')\n",
    "print('Error: ', rel_error(out, correct_out))\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward method of ReLU layer:\n",
      "dx error:  3.2756263483625388e-12\n"
     ]
    }
   ],
   "source": [
    "relu = layer.ReLU()\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(relu.forward, x, dout)\n",
    "\n",
    "relu.forward(x)\n",
    "dx = relu.backward(dout)\n",
    "\n",
    "# The error should be around 3e-12\n",
    "print('Testing backward method of ReLU layer:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Softmax classifier \n",
    "\n",
    "In multi-class classification task, as we've seen in the class, the softmax loss function is utilized. \n",
    "Practically, at the final layer of the network, instead of the standard activation, we utilize softmax function to turn the likelihood of each class into class probabilities. Then, we utilize the cross-entropy loss as the data loss. Below, you implement and return only the data loss component in your overall loss. \n",
    "*** Implement your loss computation in the function \"loss\" of the layer.py ***\n",
    "\n",
    "The L2 regularizer will be added by you in the Optimization phase later.\n",
    "You will write forward pass and backward pass for the softmax unit. Below, we evaluate your method by a numerical gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302478992941877\n",
      "dx error:  9.563859596159055e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "softmax = layer.Softmax()\n",
    "\n",
    "def softmax_loss (x,y):\n",
    "    probs = softmax.forward(x)\n",
    "    dx = softmax.backward(y)\n",
    "    loss = layer.loss(probs, y) \n",
    "    return loss,dx\n",
    "\n",
    "\n",
    "loss, dx = softmax_loss(x,y)\n",
    "dx_num = grad_check(lambda x: softmax_loss(x, y)[0], x)\n",
    "\n",
    "# The loss should be about 2.3\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Implement your activation (Bonus)\n",
    "Implement a novel or a recently published activation function and test its correctness below. If you used an activation from a paper, please don't forget to give a reference to it. Make sure that you have the correct implementation of the forward pass so that we can test your backward pass using a numerical gradient.\n",
    "\n",
    "Also, under this cell, write your activation mathematically and its derivative. Do not forget to use your activation in training part with the Wine data to show that it works and makes sense. You can also plot your activation for litte extra credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = layer.YourActivation()\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(act.forward, x, dout)\n",
    "\n",
    "act.forward(x)\n",
    "dx = act.backward(dout)\n",
    "\n",
    "relError = rel_error(dx_num, dx)\n",
    "print('Testing your activation:')\n",
    "print('dx error: ', relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Optimizers\n",
    "\n",
    "Implement SGD and SGDWithMomentum Strategies in `VanillaSGDOptimizer` and `SGDWithMomentum` classes. Test their correctness using cell below. \n",
    "**Do not forget to add L2 regularization to both optimizers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2: 7.06287979393689, W: [[ 0.35895462  0.15959199]\n",
      " [-4.4247551  -4.67411206]\n",
      " [-0.32742894 -1.04989877]\n",
      " [-0.42567003 -0.3774745 ]\n",
      " [-2.10289768 -3.03789874]\n",
      " [-0.62955362 -0.93222527]\n",
      " [-0.39180055 -0.11318631]\n",
      " [-0.33222136  0.34394803]\n",
      " [-0.51321207  0.13442902]\n",
      " [ 1.0133064   0.77294971]]\n",
      "L2: 2.717178810532411, W: [[-0.72333799 -0.45718028 -0.35107759]\n",
      " [-2.08537816 -1.321495   -0.6814232 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = layer.Model()\n",
    "layers = [layer.AffineLayer(10,2, seed=1773), layer.AffineLayer(2,3, seed=1773), layer.Softmax()]\n",
    "toyModel(layers)\n",
    "optimizer = layer.VanillaSDGOptimizer(model=toyModel, lr=1, regularization_str=1e-1)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(1,-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "expected = [ np.array([[ 0.97873084,  0.81250429],\n",
    " [-3.7373582,  -4.06007668],\n",
    " [ 0.29461562, -0.37317717],\n",
    " [ 0.23786611 , 0.27586238],\n",
    " [-1.45262147, -2.34007449],\n",
    " [ 0.03742712, -0.24127232],\n",
    " [ 0.2617457 ,  0.51694319],\n",
    " [ 0.35243035,  0.96434886],\n",
    " [ 0.17950643,  0.76174137],\n",
    " [ 1.62739663,  1.42935729]]),\n",
    "np.array([-0.23634795, -0.22072128]),\n",
    "np.array([[-0.53813187, -0.23883808, -0.09825078],\n",
    " [-1.90591288, -1.13402054, -0.4392717 ]]),\n",
    "np.array([-0.34588157, -0.00713497,  0.35301654])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Testing Weights of 0th layer\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c55667ad74c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing biases of {}th layer'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;36m1e-6\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrelError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append(toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "for i in range(4):\n",
    "    relError = rel_error(student_out[i], expected[i])\n",
    "    print(relError)\n",
    "    if i % 2 == 0:\n",
    "        print('Testing Weights of {}th layer'.format(i%2))\n",
    "    else:\n",
    "        print('Testing biases of {}th layer'.format(i%2))\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = layer.Model()\n",
    "layers = [layer.AffineLayer(10,2, seed=1773), layer.AffineLayer(2,3, seed=1773), layer.Softmax()]\n",
    "toyModel(layers)\n",
    "optimizer = layer.SGDWithMomentum(model=toyModel, lr=1, regularization_str=1e-1, mu=.5)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(1,-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "expected = [np.array([[ 0.97873084,  0.81250429],\n",
    "        [-3.7373582 , -4.06007668],\n",
    "        [ 0.29461562, -0.37317717],\n",
    "        [ 0.23786611,  0.27586238],\n",
    "        [-1.45262147, -2.34007449],\n",
    "        [ 0.03742712, -0.24127232],\n",
    "        [ 0.2617457 ,  0.51694319],\n",
    "        [ 0.35243035,  0.96434886],\n",
    "        [ 0.17950643,  0.76174137],\n",
    "        [ 1.62739663,  1.42935729]]),\n",
    " np.array([-0.23634795, -0.22072128]),\n",
    " np.array([[-0.53813187, -0.23883808, -0.09825078],\n",
    "        [-1.90591288, -1.13402054, -0.4392717 ]]),\n",
    " np.array([-0.34588157, -0.00713497,  0.35301654])]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append( toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "for i in range(4):\n",
    "    relError = rel_error(student_out[i], expected[i])\n",
    "    if i % 2 == 0:\n",
    "        print('Testing Weights of {}th layer'.format(i%2))\n",
    "    else:\n",
    "        print('Testing biases of {}th layer'.format(i%2))\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Build your own model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example which is implemented using previously defined API. In this example, you will use the widely known Wine dataset (https://archive.ics.uci.edu/ml/datasets/wine). Each instance has 13 features as the chemical analysis of wines and you will classify the data where the class number is 3 and each class represents different origin of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_wine  # Load dataset\n",
    "data = load_wine()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names) # Before training, understand your data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.data, data.target # Get the features and the corresponding classes\n",
    "model = layer.Model() # Create a model instance\n",
    " \n",
    "# Wine dataset has 13 features, so the input size of first layer is 13. We have 3 classes, so size of last hidden is 3. \n",
    "# Each neuron corresponds the likelihood of a class, named P(y=neuron_index|x), where y is class label \n",
    "# and x is features given.\n",
    "layers = [layer.AffineLayer(13,64), layer.ReLU(), layer.AffineLayer(64,3), layer.Softmax()]\n",
    "\n",
    "model(layers) # Load layers to model object\n",
    "predictions  = np.ones(178) # Number of instances in the Wine data is 178\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Shuffle dataset\n",
    "def create_permutation(x, y):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    return x[perm], y[perm]\n",
    "\n",
    "def train_test_split(X, y, ratio=.2):\n",
    "    X, y = create_permutation(X, y)\n",
    "    split_index =  int(len(X) * (1-ratio))\n",
    "    X_train, y_train = X[:split_index], y[:split_index]\n",
    "    X_test, y_test = X[split_index:], y[split_index:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "# Options\n",
    "preprocessing_on = True\n",
    "shuffle_on_each_epoch = True\n",
    "regularization_strength = 0\n",
    "n_epochs = 1200\n",
    "train_test_split_ratio = .2\n",
    "print_every = 50\n",
    "test_every = 200\n",
    "if preprocessing_on:\n",
    "    X = preprocessing.scale(X)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "\n",
    "optimizer = layer.SGDWithMomentum(model,lr=1e-1, regularization_str=regularization_strength)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if shuffle_on_each_epoch:\n",
    "        X_train, y_train = create_permutation(X_train, y_train)\n",
    "    softmax_out = model.forward(X_train)\n",
    "\n",
    "    predictions = np.argmax(softmax_out, axis=1)\n",
    "    train_acc = np.mean(predictions == y_train)\n",
    "    loss = layer.loss(softmax_out, y_train)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, loss, train_acc))\n",
    "    \n",
    "    model.backward(y_train)\n",
    "    optimizer.optimize()\n",
    "    \n",
    "    if epoch % test_every == 0:\n",
    "        softmax_out = model.forward(X_test)\n",
    "        predictions = np.argmax(softmax_out, axis=1)\n",
    "        loss = layer.loss(softmax_out, y_test)\n",
    "        test_acc = np.mean(predictions == y_test)\n",
    "        test_losses.append(loss)\n",
    "        test_accs.append([test_acc for i in range(test_every)])\n",
    "        print(\"Epoch: {}, Test Loss: {}, Test Accuracy: {}\".format(epoch, loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Plot the training and test loss curves for diagnostics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II - Book Genre Classification\n",
    "\n",
    "Now, in this part, you will work with text data (https://arxiv.org/pdf/1610.09204.pdf) for book genre analysis. Originally, the dataset is used for book genre classification by the book cover image. In this part, you will classify the books into their genres by their titles. The total number of genres for the books to be classified into is 32.\n",
    "\n",
    "Below, we already implemented the preprocessing codes fro the data. Run the below cells and load the text data \"book32-listing.csv\" into an appropriate form. You will need to use batch-wise optimizer since it is almost impossible to fit all the data at once.\n",
    "\n",
    "**IMPORTANT: You are NOT allowed to use sklearn or any other implementations for the learning part\n",
    ". You are ALLOWED ONLY TO USE your own implementation from the above steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv into a data frame\n",
    "csv = 'book32-listing.csv'\n",
    "all_data = pd.read_csv(csv, encoding = 'ISO-8859-1', index_col=0)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we only care about the Title, Author and Class columns, we will extract them and shuffle the data\n",
    "# We can enrich the feature representation by including the Author information\n",
    "from sklearn.utils import shuffle\n",
    "data = all_data[['Title', 'Author', 'Class']]\n",
    "data['Text'] = data['Title'].astype(str) + ' ' + data['Author'].astype(str)\n",
    "data = data[['Text', 'Class']]\n",
    "data = shuffle(data, random_state=42)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will use some very basic text cleaning steps \n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords') # After you download the data, you can comment this line \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Stopwords carry far less meaning than other keywords in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # Remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    # Remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # Convert text to lowercase \n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "data['Text'] = data['Text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will extract features from the text and split the data into training, validation and test sets\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500) \n",
    "# You can change the max_features if you encounter a memory error, but do not make it too small\n",
    "\n",
    "x_train_series, y_train = data['Text'][:150000], data['Class'][:150000] # 150K train\n",
    "x_val_series, y_val = data['Text'][150000:180000], data['Class'][150000:180000] # 30K val\n",
    "x_test_series, y_test = data['Text'][180000:], data['Class'][180000:] # ~30K test\n",
    "\n",
    "x_train = np.array(vectorizer.fit_transform(x_train_series).todense())\n",
    "x_val = np.array(vectorizer.transform(x_val_series).todense())\n",
    "x_test = np.array(vectorizer.transform(x_test_series).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. You will use your implementations (layers.py) below to carry out the book genre classification. Construct your model with all its layers in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Plot histogram of the weights of affine layers to see whether the weights vanish or not and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run diagnostics of your model : Try different hyperparameter settings such as number of layers in your model, learning rate, regularization parameter and such.  Avoid overfitting and underfitting as much as possible. We expect you to get at least 50% test accuracy with your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Plot the training and validation losses versus number of iterations, as you vary the regularization parameter lambda with different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot the training and validation losses as you vary the Learning Parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use two different optimizers: Mini-batch SGD and Mini-batch SGD with Momentum, and plot training and validation losses versus Iteration numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Finally, fix your model and hyperparameters according to your observations above. Plot accuracy of your classification for training and validation sets, and print your test accuracy. Remember that the test accuracy shoud be at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
