{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Fully Connected Neural Networks\n",
    "\n",
    "We covered artificial neural networks with multiple hidden layers in class. In this assignment, you will implement Fully Connected Neural Network (FCN) components in order to perform a supervised classification task.\n",
    "\n",
    "The dataset you are going to work with are : (i) for development of your code, you will use Wine dataset for classification; (ii) for actual training and testing of your implementation in this assignment, the actual dataset will be Book Genre Classification data. You will be performing a genre classification of books into 32 categories.\n",
    "\n",
    "Usage of any built-in functions for code parts that you are asked to write are not allowed. We provide a skeleton code on which to build on your own architecture. In the Layer class, there are two important methods, named as forward and backward. Almost everything you will use in this assignment is derived from this class. We will follow PyTorch-like architecture in the skeleton code.\n",
    "\n",
    "**Please do not modify the following cells, except the book genre classification cell. We will use them for the evaluation of your homeworks. **\n",
    "\n",
    "**You should modify and fill in the code under blg561/layers.py, which includes functions such as layer.NNLayer.* ...**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from blg561e.layer import layer\n",
    "from blg561e.checks import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To auto-reload your modules from the *.py files, re run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In the `Layer` class, there are two important methods, named as `forward` and `backward`. Almost everything you will use in this assignment is derived from this class. You will be programming in Python language.\n",
    "\n",
    "**Don't forget to test your implementation by using the cells below!**\n",
    "\n",
    "\n",
    "\n",
    "### a. Affine Layer\n",
    "\n",
    "In this layer, we basically implement the hidden layers of neural nets. Each neuron (building block of neural networks) is a just logistic regression classifier itself, but stacking these neurons make them powerful to implement any function.\n",
    "We are going to implement our affine layer \n",
    "\n",
    "Go under blg561e/layer.py and find Affine class. Implement the forward pass for Affine layer which is formulated as follows:\n",
    "\n",
    "$ z = W x + b $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward method of affine layer:\n",
      "difference:  8.825372662436368e-08\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 10\n",
    "input_shape = (4, 7, 2)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layer.AffineLayer(input_size, weight_size)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "affineLayer.W = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "affineLayer.b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out = affineLayer.forward(x)\n",
    "correct_out = np.array([[-0.34448963, -0.15630714,  0.03187535],\n",
    "       [-0.18626697,  0.0119934 ,  0.21025377],\n",
    "       [-0.0280443 ,  0.18029394,  0.38863218],\n",
    "       [ 0.13017836,  0.34859447,  0.56701059],\n",
    "       [ 0.28840102,  0.51689501,  0.74538901],\n",
    "       [ 0.44662368,  0.68519555,  0.92376742],\n",
    "       [ 0.60484634,  0.85349608,  1.10214583],\n",
    "       [ 0.763069  ,  1.02179662,  1.28052425],\n",
    "       [ 0.92129166,  1.19009716,  1.45890266],\n",
    "       [ 1.07951432,  1.35839769,  1.63728107]])\n",
    "\n",
    "relError = rel_error(out, correct_out)\n",
    "\n",
    "print('Testing forward method of affine layer:')\n",
    "print('difference: ', relError)\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass : \n",
    "Go under blg561e/layer.py and find AffineLayer class. Implement the backward pass for Affine layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward method of affine layer:\n",
      "dx error:  7.882509889959262e-10\n",
      "dw error:  1.3592685518020832e-10\n",
      "db error:  1.8477112902497496e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "num_inputs = 7\n",
    "input_shape = (4, 10, 3)\n",
    "output_dim = 8\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "affineLayer = layer.AffineLayer(input_size, weight_size)\n",
    "\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "affineLayer.W = np.random.randn(6, 5)\n",
    "affineLayer.b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = grad_check(affineLayer.forward, x, dout)\n",
    "dw_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.W, dout)\n",
    "db_num = grad_check(lambda _ : affineLayer.forward(x), affineLayer.b, dout)\n",
    "\n",
    "affineLayer.forward(x)\n",
    "dx, dw, db = affineLayer.backward(dout)\n",
    "\n",
    "# Errors should be around 1e-6 at least\n",
    "print('Testing backward method of affine layer:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))\n",
    "\n",
    "assert 1e-6 > rel_error(dx_num, dx) \n",
    "assert 1e-6 > rel_error(dw_num, dw) \n",
    "assert 1e-6 > rel_error(db_num, db) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. ReLU Layer\n",
    "\n",
    "Go under `blg561e/layer.py` and find `ReLU` class. Implement the forward pass for ReLU which is basicly zeroing the negative inputs:\n",
    "\n",
    "$ ReLU(x) = max(x, 0) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward method of ReLU layer:\n",
      "Error:  0.0\n"
     ]
    }
   ],
   "source": [
    "relu = layer.ReLU()\n",
    "x = np.array([0,1,3,4,-1,2,4,1773,-1773, 1.3, .4, -.1]).reshape(3, -1)\n",
    "out = relu.forward(x)\n",
    "correct_out = np.array([[0.000, 1.000, 3.000, 4.000],\n",
    "                       [0.000, 2.000, 4.000, 1773],\n",
    "                       [0.000, 1.300, 0.4, 0]])\n",
    "\n",
    "# Compare your output with ours. \n",
    "relError = rel_error(out, correct_out)\n",
    "print('Testing forward method of ReLU layer:')\n",
    "print('Error: ', rel_error(out, correct_out))\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward method of ReLU layer:\n",
      "dx error:  3.2756263483625388e-12\n"
     ]
    }
   ],
   "source": [
    "relu = layer.ReLU()\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(relu.forward, x, dout)\n",
    "\n",
    "relu.forward(x)\n",
    "dx = relu.backward(dout)\n",
    "\n",
    "# The error should be around 3e-12\n",
    "print('Testing backward method of ReLU layer:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Softmax classifier \n",
    "\n",
    "In multi-class classification task, as we've seen in the class, the softmax loss function is utilized. \n",
    "Practically, at the final layer of the network, instead of the standard activation, we utilize softmax function to turn the likelihood of each class into class probabilities. Then, we utilize the cross-entropy loss as the data loss. Below, you implement and return only the data loss component in your overall loss. \n",
    "*** Implement your loss computation in the function \"loss\" of the layer.py ***\n",
    "\n",
    "The L2 regularizer will be added by you in the Optimization phase later.\n",
    "You will write forward pass and backward pass for the softmax unit. Below, we evaluate your method by a numerical gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.302478992941877\n",
      "dx error:  9.563859596159055e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "softmax = layer.Softmax()\n",
    "\n",
    "def softmax_loss (x,y):\n",
    "    probs = softmax.forward(x)\n",
    "    dx = softmax.backward(y)\n",
    "    loss = layer.loss(probs, y) \n",
    "    return loss,dx\n",
    "\n",
    "\n",
    "loss, dx = softmax_loss(x,y)\n",
    "dx_num = grad_check(lambda x: softmax_loss(x, y)[0], x)\n",
    "\n",
    "# The loss should be about 2.3\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Implement your activation (Bonus)\n",
    "Implement a novel or a recently published activation function and test its correctness below. If you used an activation from a paper, please don't forget to give a reference to it. Make sure that you have the correct implementation of the forward pass so that we can test your backward pass using a numerical gradient.\n",
    "\n",
    "Also, under this cell, write your activation mathematically and its derivative. Do not forget to use your activation in training part with the Wine data to show that it works and makes sense. You can also plot your activation for litte extra credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing your activation:\n",
      "dx error:  5.469181638412024e-12\n"
     ]
    }
   ],
   "source": [
    "act = layer.YourActivation()\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = grad_check(act.forward, x, dout)\n",
    "\n",
    "act.forward(x)\n",
    "dx = act.backward(dout)\n",
    "\n",
    "relError = rel_error(dx_num, dx)\n",
    "print('Testing your activation:')\n",
    "print('dx error: ', relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Optimizers\n",
    "\n",
    "Implement SGD and SGDWithMomentum Strategies in `VanillaSGDOptimizer` and `SGDWithMomentum` classes. Test their correctness using cell below. \n",
    "**Do not forget to add L2 regularization to both optimizers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = layer.Model()\n",
    "layers = [layer.AffineLayer(10,2, seed=1773), layer.AffineLayer(2,3, seed=1773), layer.Softmax()]\n",
    "toyModel(layers)\n",
    "optimizer = layer.VanillaSDGOptimizer(model=toyModel, lr=1, regularization_str=1e-1)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(1,-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "expected = [ np.array([[ 0.97873084,  0.81250429],\n",
    " [-3.7373582,  -4.06007668],\n",
    " [ 0.29461562, -0.37317717],\n",
    " [ 0.23786611 , 0.27586238],\n",
    " [-1.45262147, -2.34007449],\n",
    " [ 0.03742712, -0.24127232],\n",
    " [ 0.2617457 ,  0.51694319],\n",
    " [ 0.35243035,  0.96434886],\n",
    " [ 0.17950643,  0.76174137],\n",
    " [ 1.62739663,  1.42935729]]),\n",
    "np.array([-0.23634795, -0.22072128]),\n",
    "np.array([[-0.53813187, -0.23883808, -0.09825078],\n",
    " [-1.90591288, -1.13402054, -0.4392717 ]]),\n",
    "np.array([-0.34588157, -0.00713497,  0.35301654])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n",
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n"
     ]
    }
   ],
   "source": [
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append(toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "for i in range(4):\n",
    "    relError = rel_error(student_out[i], expected[i])\n",
    "    if i % 2 == 0:\n",
    "        print('Testing Weights of {}th layer'.format(i%2))\n",
    "    else:\n",
    "        print('Testing biases of {}th layer'.format(i%2))\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "toyModel = layer.Model()\n",
    "layers = [layer.AffineLayer(10,2, seed=1773), layer.AffineLayer(2,3, seed=1773), layer.Softmax()]\n",
    "toyModel(layers)\n",
    "optimizer = layer.SGDWithMomentum(model=toyModel, lr=1, regularization_str=1e-1, mu=.5)\n",
    "\n",
    "x = np.random.randn(3,10)\n",
    "y = np.array([0,1,2]).reshape(1,-1)\n",
    "toyModel.forward(x)\n",
    "toyModel.backward(y)\n",
    "optimizer.optimize()\n",
    "expected = [np.array([[ 0.97873084,  0.81250429],\n",
    "        [-3.7373582 , -4.06007668],\n",
    "        [ 0.29461562, -0.37317717],\n",
    "        [ 0.23786611,  0.27586238],\n",
    "        [-1.45262147, -2.34007449],\n",
    "        [ 0.03742712, -0.24127232],\n",
    "        [ 0.2617457 ,  0.51694319],\n",
    "        [ 0.35243035,  0.96434886],\n",
    "        [ 0.17950643,  0.76174137],\n",
    "        [ 1.62739663,  1.42935729]]),\n",
    " np.array([-0.23634795, -0.22072128]),\n",
    " np.array([[-0.53813187, -0.23883808, -0.09825078],\n",
    "        [-1.90591288, -1.13402054, -0.4392717 ]]),\n",
    " np.array([-0.34588157, -0.00713497,  0.35301654])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n",
      "Testing Weights of 0th layer\n",
      "Testing biases of 1th layer\n"
     ]
    }
   ],
   "source": [
    "student_out = []\n",
    "for i in range(2):\n",
    "    student_out.append(toyModel[i].W)\n",
    "    student_out.append(toyModel[i].b)\n",
    "for i in range(4):\n",
    "    relError = rel_error(student_out[i], expected[i])\n",
    "    if i % 2 == 0:\n",
    "        print('Testing Weights of {}th layer'.format(i%2))\n",
    "    else:\n",
    "        print('Testing biases of {}th layer'.format(i%2))\n",
    "    assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Build your own model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example which is implemented using previously defined API. In this example, you will use the widely known Wine dataset (https://archive.ics.uci.edu/ml/datasets/wine). Each instance has 13 features as the chemical analysis of wines and you will classify the data where the class number is 3 and each class represents different origin of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_wine  # Load dataset\n",
    "data = load_wine()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names) # Before training, understand your data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.0262720233252511, Accuracy: 0.49295774647887325\n",
      "Epoch: 0, Test Loss: 0.7287711752968171, Test Accuracy: 0.5833333333333334\n",
      "Epoch: 50, Loss: 0.16550396751971724, Accuracy: 0.9647887323943662\n",
      "Epoch: 100, Loss: 0.06830212039317207, Accuracy: 0.9929577464788732\n",
      "Epoch: 150, Loss: 0.03325450003283343, Accuracy: 0.9929577464788732\n",
      "Epoch: 200, Loss: 0.023484579452449705, Accuracy: 0.9929577464788732\n",
      "Epoch: 200, Test Loss: 0.015832799598586127, Test Accuracy: 1.0\n",
      "Epoch: 250, Loss: 0.018138392341126904, Accuracy: 0.9929577464788732\n",
      "Epoch: 300, Loss: 0.015391694217479268, Accuracy: 0.9929577464788732\n",
      "Epoch: 350, Loss: 0.013651933848381552, Accuracy: 0.9929577464788732\n",
      "Epoch: 400, Loss: 0.012428511465395987, Accuracy: 0.9929577464788732\n",
      "Epoch: 400, Test Loss: 0.008056449229067831, Test Accuracy: 1.0\n",
      "Epoch: 450, Loss: 0.011510769731594619, Accuracy: 0.9929577464788732\n",
      "Epoch: 500, Loss: 0.010784160279077465, Accuracy: 0.9929577464788732\n",
      "Epoch: 550, Loss: 0.010187936852613546, Accuracy: 0.9929577464788732\n",
      "Epoch: 600, Loss: 0.00968475420728041, Accuracy: 0.9929577464788732\n",
      "Epoch: 600, Test Loss: 0.005406105203117882, Test Accuracy: 1.0\n",
      "Epoch: 650, Loss: 0.009250619483217846, Accuracy: 0.9929577464788732\n",
      "Epoch: 700, Loss: 0.008869448445367557, Accuracy: 0.9929577464788732\n",
      "Epoch: 750, Loss: 0.008529572800641316, Accuracy: 0.9929577464788732\n",
      "Epoch: 800, Loss: 0.008222889097222134, Accuracy: 0.9929577464788732\n",
      "Epoch: 800, Test Loss: 0.003982815572570107, Test Accuracy: 1.0\n",
      "Epoch: 850, Loss: 0.00794355557067922, Accuracy: 1.0\n",
      "Epoch: 900, Loss: 0.007686978060842647, Accuracy: 1.0\n",
      "Epoch: 950, Loss: 0.007449674480656017, Accuracy: 1.0\n",
      "Epoch: 1000, Loss: 0.007228883610127226, Accuracy: 1.0\n",
      "Epoch: 1000, Test Loss: 0.003103746426592851, Test Accuracy: 1.0\n",
      "Epoch: 1050, Loss: 0.007022416058151164, Accuracy: 1.0\n",
      "Epoch: 1100, Loss: 0.006828505136096566, Accuracy: 1.0\n",
      "Epoch: 1150, Loss: 0.006645704553178151, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "X, y = data.data, data.target # Get the features and the corresponding classes\n",
    "model = layer.Model() # Create a model instance\n",
    " \n",
    "# Wine dataset has 13 features, so the input size of first layer is 13. We have 3 classes, so size of last hidden is 3. \n",
    "# Each neuron corresponds the likelihood of a class, named P(y=neuron_index|x), where y is class label \n",
    "# and x is features given.\n",
    "layers = [layer.AffineLayer(13,64), layer.ReLU(), layer.AffineLayer(64,3), layer.Softmax()]\n",
    "\n",
    "model(layers) # Load layers to model object\n",
    "predictions  = np.ones(178) # Number of instances in the Wine data is 178\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Shuffle dataset\n",
    "def create_permutation(x, y):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    return x[perm], y[perm]\n",
    "\n",
    "def train_test_split(X, y, ratio=.2):\n",
    "    X, y = create_permutation(X, y)\n",
    "    split_index =  int(len(X) * (1-ratio))\n",
    "    X_train, y_train = X[:split_index], y[:split_index]\n",
    "    X_test, y_test = X[split_index:], y[split_index:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "# Options\n",
    "preprocessing_on = True\n",
    "shuffle_on_each_epoch = True\n",
    "regularization_strength = 0\n",
    "n_epochs = 1200\n",
    "train_test_split_ratio = .2\n",
    "print_every = 50\n",
    "test_every = 200\n",
    "if preprocessing_on:\n",
    "    X = preprocessing.scale(X)\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "\n",
    "optimizer = layer.SGDWithMomentum(model,lr=1e-1, regularization_str=regularization_strength)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if shuffle_on_each_epoch:\n",
    "        X_train, y_train = create_permutation(X_train, y_train)\n",
    "    softmax_out = model.forward(X_train)\n",
    "\n",
    "    predictions = np.argmax(softmax_out, axis=1)\n",
    "    train_acc = np.mean(predictions == y_train)\n",
    "    loss = layer.loss(softmax_out, y_train)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, loss, train_acc))\n",
    "    \n",
    "    model.backward(y_train)\n",
    "    optimizer.optimize()\n",
    "    \n",
    "    if epoch % test_every == 0:\n",
    "        softmax_out = model.forward(X_test)\n",
    "        predictions = np.argmax(softmax_out, axis=1)\n",
    "        loss = layer.loss(softmax_out, y_test)\n",
    "        test_acc = np.mean(predictions == y_test)\n",
    "        test_losses.append(loss)\n",
    "        test_accs.append([test_acc for i in range(test_every)])\n",
    "        print(\"Epoch: {}, Test Loss: {}, Test Accuracy: {}\".format(epoch, loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Plot the training and test loss curves for diagnostics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "axes[0].plot(train_losses)\n",
    "axes[1].plot(np.arange(test_every, epoch + test_every, test_every), test_losses)\n",
    "axes[0].set_title(\"Train Losses\")\n",
    "axes[1].set_title(\"Test Losses\")\n",
    "for axis in axes:\n",
    "    axis.set_xlabel(\"Epochs\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II - Book Genre Classification\n",
    "\n",
    "Now, in this part, you will work with text data (https://arxiv.org/pdf/1610.09204.pdf) for book genre analysis. Originally, the dataset is used for book genre classification by the book cover image. In this part, you will classify the books into their genres by their titles. The total number of genres for the books to be classified into is 32.\n",
    "\n",
    "Below, we already implemented the preprocessing codes fro the data. Run the below cells and load the text data \"book32-listing.csv\" into an appropriate form. You will need to use batch-wise optimizer since it is almost impossible to fit all the data at once.\n",
    "\n",
    "**IMPORTANT: You are NOT allowed to use sklearn or any other implementations for the learning part\n",
    ". You are ALLOWED ONLY TO USE your own implementation from the above steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Image_link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Class</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>761183272</th>\n",
       "      <td>0761183272.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61Y5cOdH...</td>\n",
       "      <td>Mom's Family Wall Calendar 2016</td>\n",
       "      <td>Sandra Boynton</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623439671</th>\n",
       "      <td>1623439671.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61t-hrSw...</td>\n",
       "      <td>Doug the Pug 2016 Wall Calendar</td>\n",
       "      <td>Doug the Pug</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00O80WC6I</th>\n",
       "      <td>B00O80WC6I.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41X-KQqs...</td>\n",
       "      <td>Moleskine 2016 Weekly Notebook, 12M, Large, Bl...</td>\n",
       "      <td>Moleskine</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761182187</th>\n",
       "      <td>0761182187.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61j-4gxJ...</td>\n",
       "      <td>365 Cats Color Page-A-Day Calendar 2016</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578052084</th>\n",
       "      <td>1578052084.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51Ry4Tsq...</td>\n",
       "      <td>Sierra Club Engagement Calendar 2016</td>\n",
       "      <td>Sierra Club</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Image                                         Image_link  \\\n",
       "Id                                                                              \n",
       "761183272   0761183272.jpg  http://ecx.images-amazon.com/images/I/61Y5cOdH...   \n",
       "1623439671  1623439671.jpg  http://ecx.images-amazon.com/images/I/61t-hrSw...   \n",
       "B00O80WC6I  B00O80WC6I.jpg  http://ecx.images-amazon.com/images/I/41X-KQqs...   \n",
       "761182187   0761182187.jpg  http://ecx.images-amazon.com/images/I/61j-4gxJ...   \n",
       "1578052084  1578052084.jpg  http://ecx.images-amazon.com/images/I/51Ry4Tsq...   \n",
       "\n",
       "                                                        Title  \\\n",
       "Id                                                              \n",
       "761183272                     Mom's Family Wall Calendar 2016   \n",
       "1623439671                    Doug the Pug 2016 Wall Calendar   \n",
       "B00O80WC6I  Moleskine 2016 Weekly Notebook, 12M, Large, Bl...   \n",
       "761182187             365 Cats Color Page-A-Day Calendar 2016   \n",
       "1578052084               Sierra Club Engagement Calendar 2016   \n",
       "\n",
       "                        Author  Class      Genre  \n",
       "Id                                                \n",
       "761183272       Sandra Boynton      3  Calendars  \n",
       "1623439671        Doug the Pug      3  Calendars  \n",
       "B00O80WC6I           Moleskine      3  Calendars  \n",
       "761182187   Workman Publishing      3  Calendars  \n",
       "1578052084         Sierra Club      3  Calendars  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv into a data frame\n",
    "csv = 'book32-listing.csv'\n",
    "all_data = pd.read_csv(csv, encoding = 'ISO-8859-1', index_col=0)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batuhanfaik/anaconda3/envs/jupyter/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471839655</th>\n",
       "      <td>Fundamentals of Photonics (Wiley Series in Pur...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604691956</th>\n",
       "      <td>50 Beautiful Deer-Resistant Plants: The Pretti...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62237330</th>\n",
       "      <td>Eric: A Novel of Discworld Terry Pratchett</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472051849</th>\n",
       "      <td>The North Country Trail: The Best Walks, Hikes...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806983590</th>\n",
       "      <td>The Rug Hook Book: Techniques, Projects And Pa...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Text  Class\n",
       "Id                                                                  \n",
       "471839655   Fundamentals of Photonics (Wiley Series in Pur...     23\n",
       "1604691956  50 Beautiful Deer-Resistant Plants: The Pretti...      8\n",
       "62237330           Eric: A Novel of Discworld Terry Pratchett     24\n",
       "472051849   The North Country Trail: The Best Walks, Hikes...     29\n",
       "806983590   The Rug Hook Book: Techniques, Projects And Pa...      8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we only care about the Title, Author and Class columns, we will extract them and shuffle the data\n",
    "# We can enrich the feature representation by including the Author information\n",
    "from sklearn.utils import shuffle\n",
    "data = all_data[['Title', 'Author', 'Class']]\n",
    "data['Text'] = data['Title'].astype(str) + ' ' + data['Author'].astype(str)\n",
    "data = data[['Text', 'Class']]\n",
    "data = shuffle(data, random_state=42)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471839655</th>\n",
       "      <td>fundamentals photonics wiley series pure appli...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604691956</th>\n",
       "      <td>beautiful deer resistant plants prettiest annu...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62237330</th>\n",
       "      <td>eric novel discworld terry pratchett</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472051849</th>\n",
       "      <td>north country trail best walks hikes backpacki...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806983590</th>\n",
       "      <td>rug hook book techniques projects patterns eas...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Text  Class\n",
       "Id                                                                  \n",
       "471839655   fundamentals photonics wiley series pure appli...     23\n",
       "1604691956  beautiful deer resistant plants prettiest annu...      8\n",
       "62237330                 eric novel discworld terry pratchett     24\n",
       "472051849   north country trail best walks hikes backpacki...     29\n",
       "806983590   rug hook book techniques projects patterns eas...      8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we will use some very basic text cleaning steps \n",
    "import nltk\n",
    "import re\n",
    "# nltk.download('stopwords') # After you download the data, you can comment this line \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Stopwords carry far less meaning than other keywords in the text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # Remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    # Remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # Convert text to lowercase \n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "data['Text'] = data['Text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will extract features from the text and split the data into training, validation and test sets\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500) \n",
    "# You can change the max_features if you encounter a memory error, but do not make it too small\n",
    "\n",
    "x_train_series, y_train = data['Text'][:150000], data['Class'][:150000] # 150K train\n",
    "x_val_series, y_val = data['Text'][150000:180000], data['Class'][150000:180000] # 30K val\n",
    "x_test_series, y_test = data['Text'][180000:], data['Class'][180000:] # ~30K test\n",
    "\n",
    "x_train = np.array(vectorizer.fit_transform(x_train_series).todense())\n",
    "x_val = np.array(vectorizer.transform(x_val_series).todense())\n",
    "x_test = np.array(vectorizer.transform(x_test_series).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. You will use your implementations (layers.py) below to carry out the book genre classification. Construct your model with all its layers in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: inf, Accuracy: 0.0078125\n",
      "Iteration: 200, Loss: 3.3579088209476087, Accuracy: 0.0703125\n",
      "Iteration: 400, Loss: 3.3313332579244403, Accuracy: 0.08203125\n",
      "Epoch: 0, Loss: 3.272640349175894, Accuracy: 0.08333333333333333\n",
      "Epoch: 0, Val Loss: 3.469320014111462, Test Accuracy: 0.05846666666666667\n",
      "Iteration: 0, Loss: 3.298610521805289, Accuracy: 0.08203125\n",
      "Iteration: 200, Loss: 3.2930137357342355, Accuracy: 0.07421875\n",
      "Iteration: 400, Loss: 3.3165789105206986, Accuracy: 0.05859375\n",
      "Iteration: 0, Loss: 3.278520419872883, Accuracy: 0.0625\n",
      "Iteration: 200, Loss: 3.1951287559263637, Accuracy: 0.11328125\n",
      "Iteration: 400, Loss: 3.2788710907330376, Accuracy: 0.09375\n",
      "Epoch: 2, Loss: 3.2678478086676077, Accuracy: 0.09583333333333334\n",
      "Epoch: 2, Val Loss: 3.469320014111462, Test Accuracy: 0.05846666666666667\n",
      "Iteration: 0, Loss: 3.3340584027569964, Accuracy: 0.08984375\n",
      "Iteration: 200, Loss: 3.31706308375716, Accuracy: 0.08984375\n",
      "Iteration: 400, Loss: 3.303881266525923, Accuracy: 0.08203125\n",
      "Iteration: 0, Loss: 3.24666529324686, Accuracy: 0.078125\n",
      "Iteration: 200, Loss: 3.2774805923344705, Accuracy: 0.08203125\n",
      "Iteration: 400, Loss: 3.30813310342847, Accuracy: 0.08984375\n",
      "Epoch: 4, Loss: 3.2137444147070626, Accuracy: 0.07083333333333333\n",
      "Epoch: 4, Val Loss: 3.469320014111462, Test Accuracy: 0.05846666666666667\n",
      "Iteration: 0, Loss: 3.277720463811212, Accuracy: 0.07421875\n",
      "Iteration: 200, Loss: 3.2494265747554962, Accuracy: 0.078125\n",
      "Iteration: 400, Loss: 3.3989697942057746, Accuracy: 0.0703125\n",
      "Iteration: 0, Loss: 3.2757312520587236, Accuracy: 0.08984375\n",
      "Iteration: 200, Loss: 3.282014443883172, Accuracy: 0.0546875\n",
      "Iteration: 400, Loss: 3.3212641267898477, Accuracy: 0.09375\n",
      "Epoch: 6, Loss: 3.2306622388436574, Accuracy: 0.1\n",
      "Epoch: 6, Val Loss: 3.469320014111462, Test Accuracy: 0.05846666666666667\n",
      "Iteration: 0, Loss: 3.2918565290018162, Accuracy: 0.0859375\n",
      "Iteration: 200, Loss: 3.2540114239209514, Accuracy: 0.109375\n",
      "Iteration: 400, Loss: 3.2530894632031258, Accuracy: 0.08984375\n",
      "Iteration: 0, Loss: 3.2531815263858865, Accuracy: 0.1015625\n",
      "Iteration: 200, Loss: 3.3005490087947673, Accuracy: 0.078125\n",
      "Iteration: 400, Loss: 3.269910170990183, Accuracy: 0.0703125\n",
      "Epoch: 8, Loss: 3.2403223663646843, Accuracy: 0.09166666666666666\n",
      "Epoch: 8, Val Loss: 3.469320014111462, Test Accuracy: 0.05846666666666667\n",
      "Iteration: 0, Loss: 3.3047232881304085, Accuracy: 0.1015625\n",
      "Iteration: 200, Loss: 3.2142790553811014, Accuracy: 0.109375\n",
      "Iteration: 400, Loss: 3.2701793346593186, Accuracy: 0.08203125\n",
      "Iteration: 0, Loss: 3.2907468177315735, Accuracy: 0.08984375\n",
      "Iteration: 200, Loss: 3.3011247503340613, Accuracy: 0.078125\n",
      "Iteration: 400, Loss: 3.3311530133419422, Accuracy: 0.0859375\n",
      "Epoch: 10, Loss: 3.2665198051777886, Accuracy: 0.07083333333333333\n",
      "Epoch: 10, Val Loss: 3.469320014111462, Test Accuracy: 0.05846666666666667\n",
      "Iteration: 0, Loss: 3.3315715598733884, Accuracy: 0.078125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-baeddcbdaf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0msoftmax_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/deep_learning/hw1/blg561e/layer/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/deep_learning/hw1/blg561e/layer/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mvectorized_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Do the affine transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Save x for using in backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_features = x_train.shape[-1]\n",
    "num_data_points = len(data[\"Text\"])\n",
    "num_classes = len(np.unique(y_train))    # Number of classes is predetermined via the dataset, however we can obtain it by analyzing the data\n",
    "book_model = layer.Model()\n",
    "layers = [\n",
    "    layer.AffineLayer(num_features, 2048),\n",
    "    layer.ReLU(),\n",
    "    layer.AffineLayer(2048, 2048),\n",
    "    layer.ReLU(),\n",
    "    layer.AffineLayer(2048, 1024),\n",
    "    layer.ReLU(),\n",
    "    layer.AffineLayer(1024, 256),\n",
    "    layer.ReLU(),\n",
    "    layer.AffineLayer(256, 64),\n",
    "    layer.ReLU(),\n",
    "    layer.AffineLayer(64, num_classes),\n",
    "    layer.Softmax()]\n",
    "book_model(layers)\n",
    "model_grads = [(index, layer_with_weights.dW, layer_with_weights.db) for index, layer_with_weights in enumerate(book_model.layers) if isinstance(layer_with_weights, layer.LayerWithWeights)]\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Options\n",
    "shuffle_on_each_epoch = True\n",
    "mini_batch_size = 128\n",
    "regularization_strength = 0\n",
    "momentum = 0.9\n",
    "n_epochs = 6\n",
    "print_every = 2\n",
    "val_every = 2\n",
    "\n",
    "# predictions = np.ones(num_data_points)\n",
    "predictions = np.ones(mini_batch_size)\n",
    "optimizer = layer.SGDWithMomentum(book_model, lr=1e-1, regularization_str=regularization_strength, mu=momentum)\n",
    "\n",
    "with np.errstate(divide='ignore'):    # Ignore the divide by zero error\n",
    "    for epoch in range(n_epochs):\n",
    "        if shuffle_on_each_epoch:\n",
    "            x_train, y_train = create_permutation(x_train, y_train)\n",
    "            \n",
    "        for iteration in range((num_data_points + mini_batch_size - 1) // mini_batch_size):\n",
    "            x_train_ = x_train[mini_batch_size*iteration:mini_batch_size*(iteration+1)]\n",
    "            y_train_ = y_train[mini_batch_size*iteration:mini_batch_size*(iteration+1)]\n",
    "\n",
    "            if len(x_train_) == 0:\n",
    "                break\n",
    "            \n",
    "            softmax_out = book_model.forward(x_train_)\n",
    "\n",
    "            predictions = np.argmax(softmax_out, axis=1)\n",
    "            train_acc = np.mean(predictions == y_train_)\n",
    "            loss = layer.loss(softmax_out, y_train_)\n",
    "\n",
    "            train_accs.append(train_acc)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            book_model.backward(y_train_)\n",
    "            optimizer.optimize()\n",
    "            \n",
    "            # Zero out gradients\n",
    "            for grads in model_grads:\n",
    "                book_model.layers[grads[0]].dW, book_model.layers[grads[0]].db = np.zeros(grads[1].shape), np.zeros(grads[2].shape)\n",
    "            \n",
    "            if iteration % 200 == 0:\n",
    "                print(\"Iteration: {}, Loss: {}, Accuracy: {}\".format(iteration, loss, train_acc))\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch, loss, train_acc))\n",
    "\n",
    "        if epoch % val_every == 0:\n",
    "            softmax_out = model.forward(x_val)\n",
    "            predictions = np.argmax(softmax_out, axis=1)\n",
    "            loss = layer.loss(softmax_out, y_val)\n",
    "            val_acc = np.mean(predictions == y_val)\n",
    "            val_losses.append(loss)\n",
    "            val_accs.append([val_acc for i in range(val_every)])\n",
    "            print(\"Epoch: {}, Val Loss: {}, Test Accuracy: {}\".format(epoch, loss, val_acc))\n",
    "            \n",
    "    # Test the model\n",
    "    softmax_out = model.forward(x_test)\n",
    "    predictions = np.argmax(softmax_out, axis=1)\n",
    "    loss = layer.loss(softmax_out, y_test)\n",
    "    test_acc = np.mean(predictions == y_test)\n",
    "    print(\"Test Loss: {}, Test Accuracy: {}\".format(loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Plot histogram of the weights of affine layers to see whether the weights vanish or not and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = [layer_with_weights.W for layer_with_weights in model.layers if isinstance(layer_with_weights, layer.LayerWithWeights)]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(model_weights), figsize=(25, 4))\n",
    "for index, weights in enumerate(model_weights):\n",
    "    print(\"Shape of Weights at Layer {}: {}\".format(index + 1, weights.shape))\n",
    "    axes[index].hist(weights)\n",
    "    axes[index].set_title(\"Hist. of Layer {}\".format(index + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing weights seems to not be a problem in this model. Because ReLU activation is applied after each affine layer with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run diagnostics of your model : Try different hyperparameter settings such as number of layers in your model, learning rate, regularization parameter and such.  Avoid overfitting and underfitting as much as possible. We expect you to get at least 50% test accuracy with your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Plot the training and validation losses versus number of iterations, as you vary the regularization parameter lambda with different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot the training and validation losses as you vary the Learning Parameter alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use two different optimizers: Mini-batch SGD and Mini-batch SGD with Momentum, and plot training and validation losses versus Iteration numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Finally, fix your model and hyperparameters according to your observations above. Plot accuracy of your classification for training and validation sets, and print your test accuracy. Remember that the test accuracy shoud be at least 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
